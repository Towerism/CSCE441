* CSCE 441-500 
Notes for Computer Graphics
* Course info
- Website [[http://faculty.cs.tamu.edu/jchai/csce441_2016spring]]
- Lecture is more important than the textbook
** Staff
- Dr. Jinxian Chai HRBB 527D (MW 1:30-2:30pm) also by appointment
- Grader: Chaitanya Visveswara (chaitu236@tamu.edu)
- TAs Jianjie Zhang & Fuhao Shi HRBB 505 (jjzhang10@tamu.edu)
- Send emails to both TA and Prof
** Submissions
- CSNet
- Due midnight on day specified
- Source and Win32 executable
- Comment your unreadable code
- No team coding
** Grading
- 60% Assignments
- 15% Midterm
- 25% Final
** Applications
*** Games
*** Movies
*** Visualization
*** Industrial Design
-* Virtual cars,
-* Environments,
-* Aircraft, etc
*** Human computer interactions
-* Microsoft's Kinect for Xbox 360 and Xbox One
*** Realworld Modeling
-* Google Earth
-* Build a model, then render an image from a particular (perhaps different) viewpoint
*** 3D Printing
-* Design 3D object in virtual environment, then print it
*** Visual Data Processing
- Image and Video processing
- Manipulating objects in a virtual space
  - Compose images and models
  - Synthesis the missing background
  - Transform pictures
** Expect to learn
Using OpenGL in programming assignments
*** 2D Graphics
-* Drawing lines, polygons
-* Image processing
*** 3D Graphics
- Transformations
- Lighting
- Ray Tracing (popular in global lighting techniques)
- Geometric Modeling
- Splines
- Animation
** Expected to know
*** Programming Experience
A*ssignments are in C/C++
*** Simple Mathematics
** How much math?
*** General geometry/linear algebra
*** Matrices
-* Multiplication
-* Inversion
-* Determinant
*** Vectors
- Dot product,
- Cross product, linear independence
** Linear Algebra Test
- See todo for deadline
- [[http://projects.cs.tamu.edu/keyser/LAtest/]]
- Need 90% correct by deadline, or will get a 0 for the class
* Into to OpenGL
** What?
*** Computer graphics API
- Developed by SGI in 1992
- Efficient, streaming interface
- 250+ function calls for drawing 2D and 3D graphics
- Hardware independent
- OS independent
- Competes against direct3D from microsoft
  - OpenGL seems to be more popular because it is OS independent
** What it is not
- No commands for windowing
- No commands for obtaining user input
- No commands for anything except drawing on the screen
** Command Formats
- Prefix (e.g. gl)
- Initial capital letters
- Number of components
  - 2: (x, y)
  - 3: (x, y, z)
  - 4: (x, y, z, w)
- Data type
- Vector
** Geogetric Primitives
See documentation for better descriptions
- GL_POINTS
- GL_TRIANGLE_STRIP
- GL_TRIANGLES
  - Each triad of vertices defines a triangle
- GL_LINES
  - Each pair of vertices constitutes a line
- GL_LINE_STRIP
  - Connect each vertex to the previous one
- GL_TRIANBLE_FAN
- GL_LINE_LOOP
  - Like STRIP except the first and last points are connected
- GL_QUADS
- GL_QUAD_STRIP
- GL_POLYGON
* Related Libraries
** GLU (OpenGL Utility Library)
- Part of OpenGL
- Provides higher-level drawing routines such as
  - Spheres,
  - NURBS,
  - Tesselators,
  - Quadric shapes, etc.
** GLUT (GLU Toolkit)
- perform system level IO with host os
- cross platform
- protable window API
* Scan Conversion of Lines
** Line-drawing using "Paint"
** How can we represent and display images
*** Image representation
**** An image is a 2D rectilinear array of pixels
- width*height array where each entry of the array stores a single pixel
- each pixel stores color information
*** Displays
  - LCD
- Oculus
- Smartphone
**** Pixels
- Smallest element of picture
- Integer position (i, j)
- Color information (r, g, b)
***** Luminance pixels
- Gray-scale images (intensity images)
- 0-1.0 or 0-255
- 8 bits per pixel (bpp)
***** Red, green, pixels (RGB)
- Color images
- Each channel: 0-1.0 or 0-255
- 24 bpp
** Digital Scan Conversion
- Convert graphics output primitives into a set of pixel values for storage in
  the frame buffer
- Store image info in the frame buffer
- Display on screen based on what is stored in frame buffer
** How to draw a line
*** Problem
- Given two points (P, Q) on the screen (with int coordinates) determine which
  pixels should be drawn to display a unit width line
**** Idea is to interpolate pixel values through rounding based on a perfect straight-through line (DDA)
***** Special lines
- Horizontal (y is constant)
- Vertical (x is constant)
- Diagonal (increment both)
***** Arbitrary lines
****** Slope-intercept equation for a line
$y = mx + b$
****** Focus on line with slope between 0 to 1
- Manipulate line equation to satisfy this property
*** Digital DIfferential Analyzer (DDA)
- Sample unit intervals in one coordinate
  - e.g. inc x by 1, inc y by m
- Find nearest pixel for each sampled point
**** Limitations
- Floating point operations (slow)
- Rounding (slow)
- Can we do better?
*** Midpoint algorithm
- Next pixel to draw is always East or Northeast from the current pixel
- Given a point, determinte whether the next pixel is E or NE
- Is the line above or below the midpoint (x + 1, y + 1/2)
  - Below: move E
  - Above: move NE
**** Implementation issues
***** How to eval if the midpoint is above or below the line
- properties
  - $y=mx+b (x,y)$ on the line
  - $y<mx+b (x,y)$ below the line
  - $y>mx+b (x,y)$ above the line
- Manipulate $f(x,y) = y-mx-b$ to achieve integers $c,d,e$
  - $f(x,y)=cx+dy+e$
  - if $f(x,y)<0$, then the point is below the line
  - similarly for > and =
***** How to incrementally eval this
- Incremental update to speed up the algorithm
- Fancy discrete math proofs and manipulations
- Need value of $f(x + 1, y + 1/2)$ to determine E or NE
- Build incremental algorithm
- Assume we have a value of $f(x+1, y+1/2)$
  - Find value of $f(x+2, y+1/2)$ if E is chosen
  - Find value of $f(x+2, y+3/2)$ if NE is chosen
***** How to avoid floating point ops
- Just a bunch of plugging into f(x,y), expanding, and simplifying
**** Advantages 
- Only integer operations
*** Midpoint Algorithm: Circle
- Similar idea: but the possible directions seem to change
** Required reqdings
HB 6.1-6.8 (remember ppt more important)
* Scan Conversion of Polygons
** Drawing General Polygons
*** Drawing Rectangles
**** How to avoid overlap?
- *RULE* In this class do *not* draw pixels on the *top* and *right* of the
  rectangle
- Benefit: we never draw the same pixel twice
- There are some limitations
*** How to draw every interior pixel?
- process the scan line from the bottom of the boundaries to the top
**** draw every interior pixel for each scan line
- including pixels intersecting the boundary
- you only draw pixels at "odd" intervals
  - The first interval is defined by the first and second intersection point
  - Next interval is defined by the second and third intersection point etc.
**** Once again follow the rule for avoiding overlapping polygons
**** Overview
***** Intersect scan lines with edges 
- use *coherence* to speed up
  - using the relationship between two intersecting scan lines and the polygon
    edge
***** Find ranges along x
***** Fill interior of those ranges
***** Draw odd intervals only
*** How to store polygon edges?
**** Sorted Edge Tables
- Associate polygon edges with scan lines
- Find out the low end point for each and correlate it with a corresponding
  scanline
- Store the multiple edges associated with each scanline as a linked list
- Exclude horizontal edges
*** Represent edges intersecting current scanline
**** Active Edge List
- List of all edges intersecting current scan-line sorted by their x-values
- Each edge is stored with the following information
| Edge     | Description                              |
|----------+------------------------------------------|
| maxY     | highest y-value                          |
| currentX | x-value of end-point with lowest y-value |
| xIncr    | 1 / slope                                |

*** Algorithm
#+BEGIN_SRC
line = 0
While (line < height)
  Add edges to active edge list from sorted edge table starting at line
    Table sfarting at line
  Remove edges with a maxY equal to currentX
  Fill interior pixels // draw pixels the curreent scan line
  Increment x-values on edges in Active Edge list
  Increment line
#+END_SRC
** Drawing Curved Objects
Neither mid-term or final will test on this!
*** How to draw every interior pixel?
- process the scan line from the bottom to top
- find the intersection positions between the current scan line and the
  boundaries of fill region
- use coherence properties to reduce the process
- fill interior pixel in the odd intervals
** Methods for drawing polygons or curved objects
*** Scanline conversion of polygons
*** Boundary fill (boundary is one color)
- Start with drawn boundaries and an interior point
- Recursively recolor outward from that point
  - If neighbor is different, then recolor and recur
- Everyting within the boundary is changed to that color
*** flood fill (boundary may be different colors)
- Start with a point
- Define color under that point sas the interior color
- Recursively recolor outward from that poing
  - If neighbor is interior color, then recolor and recur
- Contiguous regions are re-colored
*** Boundary vs Flood-fill
- Both start with an interior pixel
- Boundary-fill requires annotating boundary pixels while flood-fill requires
  annotating interior pixels
- Both are appealing to fill in the irregular boundaries
** Reading
- 6-10 and 10-14
* Clipping lines
** Why Clip?
- Do not want to waste time drawing objects that are outside of the viewing
  window (or clipping window)
** Clipping points
- Given a point (x, y) and clipping window (xmin, ymin), (xmax, ymax)
- Determine if the point should be drawn
** Clipping lines
- Given a line with end-points (x0, y0), (x1, y1) and clipping window (xmin,
  ymin), (xmax, ymax)
- Determine
  - whether line should be drawn
  - clipped end-points of line to draw
** Simple Alg
- If both end points inside rect, draw line
- If one end-point outside,
- Intersect line with all edges of rectangle
  - clip the line segment outside the rect, and repeat test with rest of edges
*** Intersecting two lines
- parametric representation of the lines
  - t = 0 corresponds to (x0,y0)
  - t = 1 correspons to (x1, y1)
- Have a system of parametric equations to determine if there is an
  intersection and also to calculate the point of intersection
*** Disadvantages
- Lots of intersection tests makes alg expensive
- Complicated tests to determine if intersecting rectangle
- Is there a better way?
**** Trivial accepts
- big optimization
- How can we quickly decide whether the line segment is entirely inside window
- Answer: test both endpoints
**** Trivial rejects
- How to know if a line is outside of the window
- Answer: both endpoints on wrong side of the same edge, can trivially reject
  the line
** Cohen-Sutherland
*** Classify p0, p1 using region codes c0, c1
- Every end point is assigned to a four-digit binary value, i.e. region code
- Each bit position indicates whether the point is inside or outside of a
  specific window edges
| bit 4 | bit 3  | bit 2 | bit 1 |
|-------+--------+-------+-------|
| top   | bottom | right | left  |

| 1001 | 1000 | 1010 |
| 0001 | 0000 | 0010 |
| 0101 | 0100 | 0110 |
**** If c0 ^ c1 not 0
- bitwise and 0010 ^ 1011 = 0010
- trivially reject
**** If c0 bitwise or c1 not 0
- trivially accept because both points are not on wrong side for any edge
- bitwise or 0010 op 1011 = 1011
**** Otherwise reduce to trivial cases by splitting into two segments
*** Window intersection
- Similar to simple alg
*** Extends easiy to 3D line clipping
- 27 regions
- 6 bits
*** Summarize
- Use region codes to quickly eliminate/incude lines
  - *best algorithm when trivial accepts/rejects are common*
- Must compute viewing window clipping of remaining lines
- More efficient algs exist
  - non-trivial clipping cost
** Liang-Barsky
*** Parametric definition of a line
- x = x1 udeltax
- y = y1 udeltay
- deltax = (x2-x1)
- deltay = (y2-y1), 0<=u<=1
*** Lines are oriented: classify lines as moving inside to out or outside to in
- For lines stating outside, update its starting point
- For lines starting inside, update end point
- For lines paralleling the boundaries and outside, reject it
*** Goal find range of u for which x and y both inside the viewing window
*** Advantages
- Faster than cohen-sutherland
- Extension to 3D is easy
  - parametric rep for 3D lines
  - Compute u1, u2 based on the intersection between ine and plane
** More Complex clipping windows
- The clipping could be any polygon
- Both Cohen and Liang can be extended to any polygon as well curve clipping
** Required reqdings: HB 8-5, 8-6, 8-7, and 8-9
* Clipping polygons
** Why is clippping hard?
- Consider the result of clipping a triangle
  - May be a triangle
  - Or some other polygon such as a 7-gon (7 is the max for a triangle)
- Tough cases such as concave polygons
** Sutherland-Hodgman Clipping
- *convex polygons*
*** Idea
- Apply line clipping to each edge of polygon
- But we may get unconnected lines which is undesirable
- *Basic idea* similar to line clipping
  - Clip against each edge of the window
  - This way the clipped polygon would be connected along the window edges
*** Input/Output for algorithm
- input: list of polygon vertices *in order*
- output: list of clipped polygon vertices consisting of old vertices (maybe)
  and new vertices (maybe)
*** How to clip against window edge
- Go around polygon one vertex (i.e. polygon edge) at a time
- Current vertex has position E
- Previous vertex had position S, and it has been added to the output if appropriate
- *Need to determine output vertices for each polygon lines*
**** Four cases
- both inside S to E
  - include E
- outside to inside S to E
  - include E plus intersection
- inside to outside S to E
  - include intersection
- both outside S to E
  - exclude both
*** Dealing with non-convex polygons
1. split concave polygon into two or more convex polygons (see sec 4-7)
2. use more general polygon clipper
3. perform the algorithm plus post-processing
*** Dealing with more general clipping boundaries
- Do polygon clipping against each boundary edge
*** Summary
- Works for convex input polygons
- Works for convex clipping boundaries
- Easy to pipeline for parallel processing
- Polygon from one boundary does not have to be completed before next boundary starts
** Weiler-Atherton 
- *general polygons*
*** Idea
- trace around perimeter of the fill polygon
- search for the borders that enclose a clipped fill region
*** Procedure
1. process the edges of the polygon fill area in a CCW order until an
   inside-outside pair of vertices is encountered
2. Follow the window boundaries in a CCW direction from the exit-intersection point to
   another intersection point with the polygon
   - *previously processed?*
   - if yes: go to next step
   - if no: continue processing polygon until a previously encountered point is
     encountered
3. Form the vertex list for this section of the clipped area
4. Return to exit-intersection point and continue processing polygon edges in a
   CCW order until another inside-outside pair of vertices is encountered
*** Summary
- Works for general input polygons (concave and convex)
- Handles a clipping window with any polygon shape (concave and convex)
- Can be extended to 3D
- Not as efficient as Sutherland-Hodgman
- Not as efficient as Sutherland-Hodgman
- Not easy to parallelize
** Nonlinear clipping-window boundaries
- Approximate the boundaries with straight-line sections
- Use the existing polygon clipping algorithms for clipping against a general
  polygon-shaped clipping window
** Required readings
HB 8-8
* 2D Transformations
** Point Representation
- We can use a column vector (a 2x1 matrix) to represent a 2D point (x, y).
- A general form of /linear/ transformation can be written as
\[
x' = ax + by + c
\]
or
\[
y' = dx + ey + f
\]
- Transformations can be written as a matrix, vector multiplication.
** Translate
- Re-position a point along a straight line
- Given a point (x, y), and the translation distance (tx, ty).
| x' |   | 1 | 0 | tx |   | x |
| y' | = | 0 | 1 | ty | * | y |
| 1  |   | 0 | 0 | 1  |   | 1 |
- How to translate an object with multiple vertices?
  - Translate each vertex individually
** Rotate
*** About the origin
- Default rotation center: Origin (0, 0)
- For theta
  - >0: Rotate counter clockwise
  - <0: Rotate clockwise
- Rotate (x, y) /about the origin/ by theta
  - Result: (x', y')
- Use trig to calculate the angle to rotate (x, y) to get (x', y')
- Matrix form
| x' |   | cos theta | -sin theta | 0 |   | x |
| y' | = | sin theta | cos theta  | 0 | * | y |
| 1  |   | 0         | 0          | 1 |   | 1 |
*** About any point 
- Idea
  - Translate the rotation center to the origin
  - Perform the rotation
  - Translate it back
| x' |   | 1 | 0 | px |   | cos theta | -sin theta | 0 |   | 1 | 0 | -px |   | x |
| y' | = | 0 | 1 | py | * | sin theta | cos theta  | 0 | * | 0 | 1 | -py | * | y |
| 1  |   | 0 | 0 | 1  |   | 0         | 0          | 1 |   | 0 | 0 | 1   |   | 1 |
** Scale
- Alter the size of an object by a scaling factor (Sx, Sy)
- Apply scaling to each vertex
- For now, translation will also occur
  - Consider scaling without translation
| x' |   | Sx |  0 | 0 |   | x |
| y' | = |  0 | Sy | 0 | * | y |
| 1  |   |  0 |  0 | 1 |   | 1 |
*** Without translation
| x' |   | 1 | 0 | px |   | Sx |  0 | 0 |   | 1 | 0 | -px |   | x |
| y' | = | 0 | 1 | py | * |  0 | Sy | 0 | * | 0 | 1 | -py | * | y |
| 1  |   | 0 | 0 | 1  |   |  0 |  0 | 1 |   | 0 | 0 | 1   |   | 1 |
** Shearing
- in x
| x' |   | 1 | h | 0 |   | x |
| y' | = | 0 | 1 | 0 | * | y |
| 1  |   | 0 | 0 | 1 |   | 1 |
- in y
| x' |   | 1 | 0 | 0 |   | x |
| y' | = | g | 1 | 0 | * | y |
| 1  |   | 0 | 0 | 1 |   | 1 |
*** Interesting facts
- A 2d rot is three shears
- Shearing will not change the area of the object
- Any 2d shearing can be done by a rotation, followed by a scaling, follow by a rotation.
** Reflection
- About X-axis
| x' |   | 1 |  0 | 0 |   | x |
| y' | = | 0 | -1 | 0 | * | y |
| 1  |   | 0 |  0 | 1 |   | 1 |
- About Y-axis
| x' |   | -1 | 0 | 0 |   | x |
| y' | = |  0 | 1 | 0 | * | y |
| 1  |   |  0 | 0 | 1 |   | 1 |
- About origin
| x' |   | -1 |  0 | 0 |   | x |
| y' | = |  0 | -1 | 0 | * | y |
| 1  |   |  0 |  0 | 1 |   | 1 |
*** About an arbitrary line
Idea, rotate, reflect, rotate back (similar to above arbitrary methods)
** Affine Transformation
- Translation, scaling, rotation, shearing are all affine transformations
- Affine transformation - transformed point is a linear combination of the
  original points
- Essentially using basic transformations to obtain a composite matrix to
  describe a complex transformation
*** How to find affine transformations
- How many points needed to estimate affine transformation?
- Three because you have two equations for each correspondent
- 6 unknowns
| x' |   | m11 | m12 | m13 |   | x |
| y' | = | m21 | m22 | m23 | * | y |
| 1  |   | 0   | 0   | 1   |   | 1 |
** Composing transformation
- Composing transformation - the process of applying several transformations in succession to
  form done overall trans
- The arbitrary methods above, can use pre-multiplication to get a composite
  transformation matrix
*** Order is important
- Matrix multiplication is associative
- Transformation products may not be commutative!
- Example: rotation and translation are not commutative
**** Some cases where it does not matter
- translation
- scaling
- rotation
** Why use 3x3 Matrices?
- So that we can use matrix, vector multiplication for all transformations
- This allows us to pre-multiply all the matrices together
- The point (x, y) needs to be represented as (x,y,1)
  - This is call *homogeneous coordinates*
  - How to represent a vector (v_x, v_y)?
    - (v_x, v_y, 0)
  - Remember,
    - for *point* the homogeneous coordinate is 1
    - for *vector* it is 0
** Applications
*** Animation
*** Image/object manipulation
*** Viewing transformation
* 3D Transformations
- A 3D point (x,y,z) - x,y, and z coordinates
- we will still use column vectors to represent points
- Homogeneous coordinates of a 3D point (x,y,z,1)
- Transformation will be performed using a 4x4 matrix
** Right-handed Coordinate System
$x*y=z$; $y*z=x$; $z*x=y$
** Translation
** Rotation
- 3D rotation is done around a rotation *axis*
- Fundamental rotations - rotate about x, y, or z axes
- CCW rotation is referred to as positive rotation (when you look down negative axis)
- Keep the axis of rotation constant
  - Replacement
  - I.e. treat the other two axes as if they are x and y axes in 2d rotation
*** Arbitrary axis
- Set up a transformation that superimposes rotation axis onto one coordinate axis
- Rotate about the coordinate axis
- Translate and rotate object back via inverse of the initial transformation
- The resulting composite matrix is *orthonormal*
  - column rows linearly independent *orthogonal*
  - column rows are unit vectors *normalized*
  - inverse of the matrix is its transpose
** Scaling
- Very similar to 2d transformation
** Inverse of 3D transformations
- Invert the transformation matrix
* Coordinate transformation
** Review
- Dot product: angle between two vectors
- Cross product: area determined by two vectors
** 2D Cartesian coordinate system
- Axes described by unit vectors i and j.
  - $i \cdot i = 1$
  - $j \cdot j = 1$
  - $i \cdot j = 0$
- Any 2D vector can be represented as xi + yj
- Any 2D vector starting from the origin can be described as $op = xi + yj$
*** Transform object description


- from $i'j'$ to $ij$
- Use composite matrix which performs a possible rotation and a possible
  translatio
- Build a relation using three vectors
  - from old to new origin
  - from new origin to point
  - from old origin to point
- new point given by
  - $[i' j' o']p$
*** Alternative way to look at the system
- transforming the old coordinate system to the new coordinate system
- then take the inverse transformation coordinates to achieve the old
  coordinates in the new coordinate system
** 3D Coordinate Transformation
- Once again use 4 by 4 transformation matrix to model the transformation
** Composite 2D Transformation
- Describe the model transformations in 2d objects
- Multiple coordinate transformations to model animation of a character
- *Forward kinematics function*, mapping of a local point of a character to a global
  coordinate system
*** Animate the character
- *keyframe animation*
  - manually pose the character by choosing appropriate position and angle parameters
  - linearly interpolate in between poses
  - works for any types of articulated characters
** Composite 3D Transformation
- Similarly, we can extend composition transformation from 2D to 3D
- Once again a 4x4 transformation matrix
* Hierarchical models
** Symbols and Instances
- Mose graphics API supports a few primitives:
  - sphere
  - cube
  - cylinders
- These symbols are instanced using instance/model transformation
** Instance translation
- created by modifying the model-view matrix
#+BEGIN_SRC
glMatrixMode(GL_MODELVIEW)
glLoadIdentity(...)
glTranslate(...)
glRotate(...)
glScale(...)
house()
#+END_SRC
*** Opengl implementation
- Opengl postmultiplies transformation matrices as they are called
- Each subsequent transformation call concatenates the designated transformation
  matrix on the right of the composite matrix
We must invoke the transformation in the opposite order from which they are applied
*** Consider a car model
- 2 primitives: chassis + wheel
- 5 instances: 1 chassis + 4 wheel
- Represent the car as a tree to show relationship between the parts
** Hierarchical Modeling
- Hierarchical model can be composed of instances using trees or directed
  acyclic graphs (DAGs)
- Edges contains geometric transformations
- Nodes contains geometry
- Drawing is done most efficiently using a DFS
** Articulated Models
- You can draw these models as long as
  - you know how to draw each primitive in local reference frames
  - you know how to call transformation matrices to model the relationship
    between the primitives
* 3-D Viewing
** 3D Geometry Pipeline
- Object space
- World space
- View space
- Normalized projection space
- Screen/Image space
*** OpenGL Codes
- Just as in hierarchical modeling
- Apply transformations in reverse order
- That is
  - viewport transformation
  - projection transformation
  - viewing transformation
** Rotate and translate camera to desired camera viewpoint
*** Camera coordinate
**** Canonical coordinate system
- usually right handed (looking down z axis)
- convenient for project and clipping
**** Mapping from world to eye coordinates
- eye position maps to origin
- right vector maps to x axis
- up vector maps to y axis
- back vector maps to z axis
- Then you use this mapping to use 3D coordinate translation to find the mapping
  from world to eye coordinates
*** Viewing transformation
- We have the camera in world coordinates
- We want to model translation T which takes object from world to camera
- Trick: find inverse of T taking object from camera to world
**** gluLookAt
- Need to know a few things
- Camera center (eye)
- Point on an object (of interest) to look at
- Up vector for the camera coordinate system system
***** How to properly configure up, right, and back vectors
- make sure that the right vector will be perpendicular to the up vector
- accomplish this by taking the cross product of the vector from eye origin to
  the point to look at and the up vector.
*** Projection
- General definition
  - transform points in n-space to m-space (m<n)
- In computer graphics
  - map 3D coordinates to 2D screen coordinates
**** Map 3D coordinates to 2D coordinates
***** Perspective projection
- maps points onto "view plane" along projectors emanating from "center of
- consider the projection of a 3D point on the camera plane
  - using similar triangles
  - we can compute the scale for the x and y coordinates
- transformation
  - $(x,y,z) \left arrow (-dx/z, -dy/z)$
  - remember to use homogeneous coordinates in order to turn this transformation
    into a linear transformation
***** Perspective effects
- Distant object object becomes small
- The distortion of items when viewed at an angle (spatial foreshortening)
***** Properties of Perspective Projection
****** Perspective projection is an example of projective transformation
- lines maps to lines
- parallel lines do not necessarily remain parallel
  - *vanishing points* each set of parallel points not parallel to the
    projection plane will vanish at their intersection point in the projection
- ratios are not preserved
******* Advantage: size varies inversely proportionally to distance to look more realistic
***** Parallel Projection
- Center of projection is at infinity
****** Orthographic projection
- Special case of parallel projection
- Direction of projection (DOP) perpendicular to view plane
- Depth values are all mapped to 0
****** Properties
- Not realistic looking
- Good for exact measure
- Are actually affine trasformation
  - parallel lines remain parallel
  - ratios are preserved
  - angles are often not preserved
- Often used in CAD programs
**** Perspective Projection Volume
- The center of the projection and the portion of projection plane that map to
  the final image form an infinite pyramid. The sides of pyramid called clipping
  planes
- Additional clipping planes are inserted to restrict the range of depths
  - Far clip plane
  - Near clip plane
  - Both planes will define a viewing (truncated) pyramid
    - Objects not inside the pyramid will be clipped
- The truncated pyramid maps to a cube in the normalized projection space (left-handed)
- The view space (right-handed)
***** OpenGL
- Truncated pyramid has 6 sides, defined using ~glFrustum~ function which takes
  6 parameters
  - First four parameters define the polygon dimensions for the near clipping plane
  - The second two parameters define the distances of both clipping planes from
    the eye
- ~gluPerspective~ is the other function you can use
  - Essentially calls glFrustum creating a symmetric truncated pyramid
- Obtain normalized depth values between -1 and 1
*** Viewport transformation
- In OpenGL, use ~glViewport~ function
  - specify a rectangle on screen for the viewport
  - depth values are transformed to be between 0 and 1
* Hidden Surface Removal
** Rendering Pipeline
- Modeling transformation
- Lighting
- Viewing transformation
- Project transformation
- Clipping
- Scan conversion
- Image
** Hidden Surface Removal
*** Hidden Surfaces
- Motivation: determine which pixels are visible or not visible from a certain viewpoint
**** Polygon mesh representation
- Vertex and face list
  - vertex list maps faces that contain the vertex
  - face list maps vertices that form a face to that face
*** Algorithms
**** Backface Culling
- *idea* cull triangles which are not facing the camera
***** Advantages
- Speeds up rendering by removing roughly half of polygons from scan conversion
***** Disadvantages
- Assumes closed surface with consistently oriented polygons
- Not a true hidden surface algorithm
**** Painter's Algorithm
- *Idea* similar to oil painting
  - draw background first
  - then most distant object
  - then nearer object
  - and so forth
- Sort polygons according to distance from viewer
- Draw from back (farthest) to front (nearest)
  - the entire object
- Near objects will overwrite farther ones
- Problem: objects can have a /range/ of depth, not just a single value
- Need to make sure they don't overlap for this algorithm to work
  - Might need to split up some polygons
***** Advantages
- Simple algorithm for ordering polygons
***** Disadvantages
- Splitting is not an easy task
- Sorting can also be expensive
- Redraws same pixel many times
**** Binary Space Partitioning Trees
- Basic principle: Objects in the half space opposite of the viewpoint do not
  obscure objects in the half space containing the viewpoint; thus, one can
  safely render them without covering foreground objects
***** BSP Tree
- Organize all of space (hence partition) into a binary tree
- /pre-process/ overlay a binary tree on objects in the scene
- /run-time/ correctly traversing this tree enumerates objects from back to front
  similarly to painters' algorithm
- *Idea* divide space recursively into half spaces by choosing splitting planes
  - splitting planes can be arbitrarily oriented
****** Details
- positive half-space objects are place in the left branch
- push down objects into the appropriate child branch as you go down until every
  leaf has only 1 object
- objects which are split by a half-space-line can be split into two objects
****** Summary
- Split along plane containing any polygon
- classify all polygons into positive or negative half-space of the plane
  - if a polygon intersects a plane, split it into two
- recurse down positive half-space
- recurse down negative half-space
***** Building a BSP tree for polygons
- Choose a splitting polygon
- Sort all other polygons as
  - front
  - behind
  - crossing
  - on
- Add "front" polygons to front child, "behind" to back child
- Split "crossing" polygons with infinite plane
- Add "on" polygons to root/current node
- Recur
****** Drawing:
- Basically in-order traversal
- But it depends on camera view point
- If eye is in front of plane
  - Draw "back" polygons
  - Draw "on" polygons
  - Draw "front" polygons
- If eye is behind plane
  - Draw "front" polygons
  - Draw "on" polygons
  - Draw "back" polygons
- If eye is on plane
  - Draw "front" polygons
  - Draw "back" polygons
***** Speed improvement
- Take advantage of view direction to cull away polygons behind viewer
***** Summary
****** Pros
- Simple, elegant scheme
- no depth comparisons needed
- polygons split and ordered automatically
- works for moving cameras
- only writes to frame buffer (similar to painters algorithm)
****** Cons
- Computationally intense pre-process state restricts algorithm to static scenes
- Worst-case time to construct tree: O(n^3)
- Splitting increases polygon count
  - Again O(n^3) worst case
- Redraws same pixel many times
- Choosing splitting plane not an exact science
- Not suitable for moving objects
**** Z-Buffer
***** Main idea
- Simple modification to scan-conversion
- Maintain a separate buffer storing the closest "z" value for each pixel: depth buffer
- only draw pixel if depth value is closer than stored "z" value
  - Update buffer with closest depth value
  - work in normalized coordinate space [0.0...1.0]
***** Algorithm
- Initialize the depth buffer and frame buffer for every pixel
- Process each polygon in a scene, one at a time
***** Calculate "z"
- Easy to implement for polygon surfaces using plane equation
- How can we speed up the calculation?
  - use incremental update during scan-line conversion
  - update "z" values using scan line conversion algorithm
***** Pros
- Always works. nearest object always determines the color of a pixel
- polygon drawn in any order
- commonly in hardware
***** Cons
- Needs a whole extra buffer (depth buffer)
- Requires extra storage space
- Still overdraw
***** Opengl
- In opengl, depth values are normalized to [0.0, .1.0]
- Specify initial depth-buffer value
- Activate depth-testing operations
**** Ray casting
* Color
** Human Vision
- Color/light is electromagnetic radiation within a narrow frequency band
*** Components
- Incoming light
- Human eye
** Visible Light
- Human eye can see "visible" light in frequency between 380nm-780nm (Spectral color)
- Visible light is a combination
** Spectral Energy Distribution
- A light source emits all frequencies within the visible range to produce white light
- Three different types of lights
*** Perception of Object colors
- Why does the object appear different color under different light?
**** Perceived object color depends on
- Incoming color of light
- Surface reflectance property
*** Different types of light
Daylight
Fluorescent
Incandescent
*** Ideal Color Representation
- *Unique* one-to-one mapping
- *Compact* require minimal number of bits
- *General* represent all visible light
- *Perceptually appropriate* tell us hue, luminance, purity of color
** Hue, Brightness, & Purity
- *Hue* the color of light corresponding to the dominant frequency of the color
- *Brightness* corresponds to the total light energy and can be quantified as
  the luminance of the light
- *Purity (or saturation)* describes how close a color appears to be a pure
  spectral color, such as red
** Color Representation
*** Human Vision
**** Photo-receptor cells in the retina:
***** Rods
- 120 million rods in retna
- 1000X more light sensitive than cones
- Discriminate between brightness in low illumination
- Short wave-length sensitive
***** Cones
- 6-7 million cones
- Responsible for high-resolution vision
- Discriminate colors
- Three types of color sensors (64% red, 32% green, 2% blue)
- Sensitive to any combination of three colors
*** Tristimulus of Color Theory
- Spectral-response of each of the three types of cones
- Color matching function based on RGB
  - Any spectral color can be represented as a linear combination of these
    primary colors
**** Color is psychological
- Representing color as a linear combination of red, green, and blue is related
  to cones, not physics
- Most people have the same cones, the there are some people who don't - the sky
  might not look blue to them (although they will still call it blue nonetheless)
**** Additive and Subtractive color
- Normalized weights: between 0 and 1
  - RGB color model
    - white [1 1 1]^T
    - green [0 1 0]^T
  - CMY color model (Cyan Magenta Yellow)
    - white: [0 0 0]^T
    - green: [1 0 1]^T
**** RGB color space
- Can be viewed as a 3D space
***** RGB cube
- Easy for devices
- Can represent all the colors? No
- But not perceptual
- Where is brightness, hue, and saturation?
**** Summary
- Since 3 different cones, the space of colors is 3D
- We need a way to describe color within this 3D space
- No finite set of light sources can be combined to display all possible colors
- We want something that will let us describe any visible color with *additive*
  combination of three primary (imaginary) colors
*** The CIE XYZ System
- CIE - Commision Internationale de l'Eclairage
  - International Commission on Illumination
  - Sets international standards related to light
- Defined the XYZ color system as an int'l standard in '31
- X, Y, Z are three primary colors
  - imaginary colors
  - all types of color can be represented by an additive combination of the
    three primary colors
  - Standard, but not very intuitive
  - There is more than one way to specify color
  - Variety of color models have developed to help with some specifications
  - Not possible to represent all visible color
**** Color Matching Functions
- Given an input spectrum, we want to find the X, Y, Z coordinates for that color
- Color matching functions tell how to weigh the spectrum
**** XYZ space
- The visible colors form a "cone in XYZ space
- For visible colors, X, Y, Z are all possible
- C_x, C_y, C_z are not visualizable
**** Luminance and Chromaticity
- The intensity *luminance* is just X+Y+Z
  - Scaling X, Y, Z increases intensity
  - We can separate this from the remaining part, *chromaticity*
- Color = Luminance + Chromaticity (Hue and Purity)
- Project the X+Y+Z=1 slice along the Z-axis
- Chromaticity is given by the x, y coordinates
**** Functions of Chromaticity Diagram
- Determining purity and hue (dominant wave length) for a given color
- Identify complementary colors
- Compare color gamuts generated by different primaries (e.g. on different
  devices)
***** White Point
- White: at the center of the diagram
- Approximation of average of daylight
***** Saturation/Purity
- As you move on line from white to spectral color, you increase the saturation
  of that color
- How to compute this?
  - The ratio of the magnitude of the saturation vector by the magnitude of the
    "100% saturated" vector
***** Hue
- Whats the dominant wavelength of color for which the nearest edge is not spectral?
  - Take complementary color of opposite edge
- Whats the purity?
***** Combine two colors
- Two colors, A and B
- Vary the relative intensity
- Generate any color on the line between A and B
***** Complementary colors
- Complementary colors are those that will sum to white
- The distances to white determine the amounts of each needed to produce white
  light
***** Combine three colors
- vary relative intensity
- generate any color in the triangle between them
***** Gamut
- Display devices generally have 3 colors (a few have more)
  - e.g. RGB in monitor
- The display can therefore display any color created from a combination of
  those 3
- Display range that the monitor can produce by combining its colors is called
  that display's *gamut*
- How to to find the appropriate color space to represent all visible colors
  - There is no perfect color representation
*** RGB
- Red, Green, Blue
- Common spec for most monitors
- Not standard
*** CMY
- Cyan, Magenta, Yellow
- Commonly used in printing
- Generally used in a /subtractive/ system
*** CMYK
- CMY, Black
- Comes from printing process - Since CMY combine to form black, can replace
  equal amounts of CMY with Black, saving ink
*** HSV Color Model
- Perceptually appropriate
- Nonlinear transform between the HSV and RGB space
* Image Filtering
- *filter* Process that removes from a signal some unwanted component or feature
- filtering is altering the *range* of image
- warping is altering the *domain* of image
- filtering in spatial domain involves applying a filter function to the input
  image
- *Common themes*
  - Interwindow iteration
  - Keeping pixel weights normalized so they sum to 1
** Gaussian Filtering
- the filter function is a discrete approximation to Gaussian function (with
  sigma equal to 1.0)
- filter applied by modifying surrounding pixels
*** Features
- blurs image
- preserves details only for small sigma
** Median Filtering
- for each neighbor in image sliding the window
- sort pixel values
- set the center pixel to the median
- increasing the size of the window increases blurriness
- straight edges kept
- sharp features lost
*** Features
- can remove outliers
- Window size controls size of structure
- Preserve some details but sharp corners and edges might be lost
- Blurs image
- Removes simple noise
- No details preserved
** Bilateral Filtering
- Affecting or undertaken by two sides equally
- *Property*
  - Convolution filter
  - Smooth image but preserve edges
  - Operates in the domain and range of image
- *Procedure*
  - Apply a Gaussian filter on pixel weights based on value difference
  - Smooth pixel values
*** Comments
- Can work with any reasonable distances function
- Easily extended to higher dimension signals, e.g. images, video, mesh, etc.
* Image Warping
- filtering is altering the *range* of image
- warping is altering the *domain* of image
- Can be useful for many things
  - texture mapping (apply a texture to various 3d surfaces)
  - image processing (rotation, zoom in/out etc)
  - etc
- *Transformation function* used to transform geometry of an image to desired geometry
  - Used to compute corresponding points
- *Control points* Unique points in the reference and target images. The
  coordinates of corresponding control points in images are used to determine a
  transformation function.
** Image warps
- Translation
- Rotation
- Aspect
- Affine
- Perspective
- Cylindrical
** Warping Types
They can be applied globally over a subdivision of the plane
- Piecewise affine over triangulation
- Piecewise projective over a quadrilaterization
- Piecewise bilinear over a rectangular grid
Or other, arbitrary functions can be used, e.g.
- Bieer-neely warp (popular for morphs)
- Store u(x, y) and v(x,y) in large arrays
*** Similarity
- Combination of 2-D scale, rotation, and translation transformations
- Allows a square to be transformed into any rotated rectangle
- Angle between lines is preserved
- 5 degrees of freedom
- Inverse is of same form, given by inverse of transformation matrix
*** Affine mapping
- Combination of 2-D scale, rotation, shear, and translation transformations
- Allows a square to be distorted into any parallelogram
- 6 degrees of freedom
- Inverse is of same form (is also affine). Given by inverse of transformation
  matrix
- Good when controlling a warp with triangles, since 3 points in 2D determined
  the 6 degrees of freedom
*** Projective map
- Linear numerator and denominator
- If g=h=0, then you get affine as a special case
- Allow a square to be distorted into any quadrilateral
- 8 degrees of freedom (the first two columns of the third row of the
  transformation matrix)
- Inverse is of same form
- Good when controlling a warp with quadrilaterals, since 4 points in 2D
  determine the 8 degrees of freedom
** Mapping
- Inverse mapping is better than forward mapping
  - Forward mapping requires that you splat pixels to neighboring pixels
  - Whereas with inverse mapping you simply linearly interpolate.
- Requires a re-sampling filter
- Typically you perform inverse warping followed by a re-sample
** Re-sampling
- HQ resampling requires careful use of low pass filters of variable support
- Good resampling for scale factors near 1 (not scaling up or down much) can
  be done with bilinear interpolation
- Calculated by computing weighted sum of pixel neighborhood
*** Point sampling
- Nearest neighbor
- Copy the color of the closest integer coordinate
- Fast and efficient if the target size is similar to the reference
- Otherwise, the result is chunky, aliased, or blurred
*** Bilinear Filter
- Weighted sum of four neighboring pixels
* Lighting
** In the rendering pipeline
- Occurs after Modeling transformation
- Before Viewing transformation
** Illumination
- Color is a function of how light reflects from surfaces to the eye
*** Global illumination 
- accounts for light from all sources as it is transmitted
- Light is emitted from a light source and reflected off of many surfaces onto
  other surfaces
*** Local illumination 
- only accounts for light that directly hits a surface and is transmitted to the
  eye
- does not consider light that would be reflected off of other surfaces
*** Light Sources
- Any object emitting radiant energy is a light source that contributes to the
  lighting effects for other objects in a scene
- *Point light sources* defined by its position and the color of the emitted
  light (RGB)
- *Infinitely distant light sources* basically a point light source very far from
  a scene. Defined by its direction and the color of the light (RGB)
*** Reflection Models
- *Reflection* the process by which light incident on a surface interacts with
  the surface such that it leaves on the incident side without change in
  frequency
- In general you can represent reflection properties by a reflection surface
  model
  - diffuse, or
  - specular
**** Ideal Specular
- Reflection Law
- Mirror
**** Ideal Diffuse
- Lambert's Law
- Matte
**** Specular
- Glossy
- Directional diffuse
- "In between ideal specular and diffuse"
*** Materials
- Plastic
- Metal
- Matte
** Illumination Model
*** Ambient light
$I = k_aA$
- Need to know intensity of ambient light, and
- Ambient reflection coefficient
- There are three equations (one for each of RGB)
- Reflected light for every pixel is the same, and is scaled by the ambient term
*** Diffuse light
- Assumes that light is reflected equally in all directions
- Handles both local and infinite light sources
  - Infinite distance: Incident light angle does not change (with respect to
    normal)_
  - Finite distance: must calculate the angle for each point on surface
- Need to know intensity of source,
- diffuse reflection coefficient,
- and angle between normal and direction to light
- Three equations once again that you combine for the overall
**** Lambert's law
- Intensity received is equal to the intensity of the source times the dot
  product of the incident light and the normal.
- Thus beam width divided by surface area is cosine theta
- For reflected light, simply scale by the reflection coefficient
$I = Ck_dcos(\theta) = Ck_d(L \cdot N)$
**** Total illumination
$I = k_aA+k_dC(L \cdot N)$
*** Specular Light
- Perfect, mirror-like reflection of light from surface
- Forms highlights on shiny objects
- Reflected light is mirror to incident light across the normal.
- Also takes into account the direction of the camera
- $I=Ck_scos^n(\alpha)=Ck_s(R \cdot E)^n$
  - $C$ = intensity of point light source
  - $k_s$ = specular reflection coefficient
  - $\alpha$ = angle between reflected vector ($R$) and eye ($E$)
  - $n$ = specular coefficient
**** Finding the Reflected Vector
$L_{parallel} = N\cos(\theta)=N(L \cdot N)$
$L_{perpendicular}=L-L_{parallel}$
$R=L_{parallel}-L_{perpendicular}=2L_parallel - L$
**** Total Illumination
$I=k_aA+C(k_d(L \cdot N) + k_s (R \cdot E)^n)$
*** Multiple Light Sources
- Sigma together incoming light intensities
** Light attenuation and spot light
*** Attenuation
- Decrease intensity with distance from light
- $d$ = distance to light
- $r$ = radius of attenuation for light
- for example $att(d,r)=e^{{-d^2}/r^2}$
**** For multiple light sources
- Scale the intensity of incoming light by the attenuation function for each
  corresponding light source
*** Spot lights
- Eliminate light contribution outside of a cone
- How to create spot lights?
  - If point is outside of the cone, the intensity is zero!
  - For a point inside the cone, it depends on the angle of the incident light
    to the central access
**** For multiple light sources
- Scale the intensity of incoming light by the spotlight coefficient for each
  corresponding light source
** Implementation Considerations
- $I=k_aA+C(k_d(L \cdot N) + k_s (R \cdot E)^n)$
  - If $\theta >90$, no contributions
- Typically choose $k_a+k_d+k_s \leq 1$
- Clamp each color component to $[0,1]$
** Opengl functions
- See section 17-11
- How to set up light sources
  - light source pos
  - type
  - source colors
  - radial-intensity attenuation
  - spotlights
- How to specify global lighting parameters
- How to specify surface properties
* Shading
** Problem
- Need to render each polygon efficiently
- *Simple solution* calculate illumination once per polygon.
  - Use this to approximate entire polygon
- *Less simple solution* calculate illumination for each vertex.
  - Use them to interpolate for entire polygon
- *Complex solution* interpolate normal and depth values for every interior pixel
  and use them to calculate the entire illumination
** Vertex Normals
- How to compute vertex normal given the normal of each polygon?
  - First, compute normal for each polygon
  - Next, average the normals
** Interpolation
*** Over Polygons
- Given values at vertices of polygon, how do we interpolate data over interior?
  - values could be either vertex normal or color, or even depth
  - any vertex can be represented as a linear combination of polygon vertices
  - we can interpolate values with the same weights (barycentric coordinates)
- Per-vertex shading: RGB color interpolation
- Per-Pixel shading: Normal interpolation
*** Intensity interpolation (Gouraud Shading)
- Polygon rendering can be combined with scan conversion
- Two stage interpolation:
  - First interpolate border pixels,
  - Then interpolate interior pixels from border pixels
- *Made more efficient with incremental calculation*
  - Add currentF and fIncr to the edge data struct
    - where currentF is the current feature value
    - and fIncr is how much to increment the feature value in the next scan-line
*** Interpolating Normals (Phong Shading)
- Exactly the same as colors
- Must re-normalize
- Does not produce even spacing
  - slerp
** Texture Mapping
- Geometry and lighting alone do not provide sufficient visual details
- "Paste" 2D image onto 3D surface
- Surface appears much more complex than reality
** Techniques
*** Flat Shading
- Result not desirable: not smooth
*** Gouraud (Per-Vertex) Shading
- Assume normals at vertices of polygon
  - If all normals the same, then the result is the same as flat shading
  - Determine colors at each vertex
- Use colors at vertex and interpolate for the interior of the polygon
*** Phong (Per-Pixel) Shading
- Assume normals at vertices of polygon
- Interpolate normals from vertices across polygon
- Determine color at each pixel in polygon
- Captures highlights better
- *Best result, but most expensive*
  - Recommended, especially today when GPU acceleration has gotten more
    sophisticated
* Texture Mapping
- Geometry and lighting alone do not provide sufficient visual details
- "Paste" 2D image onto 3D surface
- Surface appears much more complex than reality
- Add visual details to surfaces of objects
- When scan-converting a polygon, vary pixel colors according to values fetched
  from a texture
- It is a 3D projective projection
  - texture coordinate system to
  - image coordinate system
- Assume texture parameterized by u,v and also normalized
- During polygon drawing, lookup color from texture using interpolated texture
  coordinates
** Sampling Textures
- Nearest Neighbor
  - Simplest method
  - Very blocky, undesirable results
- Linear blending
  - Smooth appearance
- There exists more complex techniques
** Interior Points
- Interpolate the u,v coordinates of the vertices
** Combine Texture with Shading
- Multiply the mapped texture image with the illumination map
** Mip Maps
- Set of down-sampled textures
- Pick one based on size of sampling region
- Speed up rendering process
** Other uses of texture Mapping
- In general, any attribute can placed into a texture
*** Bump/Normal Mapping
- Simulate the effects of details in geometry without adding geometry
**** Different from Texture Mapping
- Texture mapping cannot simulate rough surface details
- Rough surfaces show illumination changes with the movement of the light or the
  object
  - Texture objects cannot simulate that
  - Since independent of illumination parameters
- Basic idea: perturb the normal and use the perturbed normal for illumination
  computation
- Make shading look more complicated than geometry really is
- Represent the perturbations of normals in the same way as texture image
*** Displacement Mapping
- Offset geometry in direction of normal
- Encode offset (height info) inside texture
- Used to actually change the geometry and provide more detail (especially
  silhouette)
- Usually needs a lot of vertices to create realistic surface details
**** Hardware Displacement Mapping (HDM)
- Can be hardware accelerated
- Often combines with *Tessellation* process
- *Tessellation* is a method of breaking down polygons into smaller pieces.
- *Displacement mapping* and *tessellation* is used to create 3D graphics for
  almost all the games in PlayStation 4
*** Environment Mapping
* Ray Tracing
- *Ray Tracing* A global illumination technique
- Because we only consider one ray per light source, it is just an approximation
** Essential Information
- Eye point
- Screen position/orientation
- Objects
  - Material properties
  - Reflection/Refraction coefficients
  - Index of refraction
- Light sources
** Types of Rays
*** Illumination/Shadow
- Direct lighting from light sources
**** Procedure
- A ray is cast from an object's surface towards a light
- If the light is not occluded then the light contributes to the object's
  surface color
- Otherwise, the light does not contribute to the object's surface color
- If ray hits a semi-transparent object scale the contribution of that light and
  continue to look from intersections
*** Reflective
- Light reflected by an object
- Single ray chosen is symmetric to the eye vector across the surface normal
**** Procedure
- A ray is cast from the surface of an object based on its material properties
- The contribution results in the specular reflection
*** Refraction/Transparent
- Light passing through an object
- Single ray chosen by Snell's law
**** Procedure
- Some objects are transparent or translucent
- The transmitted light also contributes to the surface color, called specular
  transmission
- The ray can be refracted based on the object's composition
** Recursive Ray Tracing
- $I=I_{direct} + \gamma_eI_{reflected} + \gamma_aI_{refracted}$
  - $I_{direct} = I_{ambient} + I_{diffuse} + I_{specular}$
  - $reflected$ and $refracted$ terms are calculated recursively
*** Procedure
- For each pixel
  - Intersect ray from eye through pixel with all objects in scene
  - Find closest (positive) intersection to eye
  - Compute lighting at intersection point
  - Recur for reflected and refracted rays (if necessary)
*** Termination Criterion
1) The ray intersects no surfaces
2) The ray intersects a light source that is not a reflecting surface
3) The tree has been generated to its maximum allowable depth
   1) I.e. the next iteration would not change the result significantly
** Three Issues
*** Ray-object intersection
**** Ray casting
- Parameterize each ray as $r(t) = c+t(p_{ij}-c)$
- Each object $O_k$ returns $t_k>0$ such that first intersection with $o_k$
  occurs at $r(t_k)$
- Question: given the set ${t_k}$, what is the first intersection point?
**** Ray-triangle intersection
- Intersect ray with plane
- Check if point is in triangle
*** Reflection direction
- Mirror-like/Shiny Objects
- $R=2(V \cdot N)N - V$
*** Refraction direction
- Bending of light cased by different speeds of light in different medium
- Each (semi-)transparent object has an index of refraction $n_i$
- Use Snell's law to find refracted vector
**** Snell's law
- $\dfrac{\sin(\theta_1)}{\sin(\theta_2)}=\dfrac{c_1}{c_2} = \dfrac{n_2}{n_1}$
- Given V and N, as well as $n_1$ and $n_2$, how to calculate R?
  - $R=\cos(\theta_2)(-N)+\sin(\theta_2)\dfrac{-V_{perp}}{|V_{perp}|}$
** Surface Intersection
*** Infinite planes
*** Spheres
*** Polygons
** Optimizations
*** Bounding Box
- Lots of rays to cast!
- Ray-surface intersections are expensive
- Associate with each object
  - Bounding box in 3d-space
- If ray doesn't intersect box, then ray doesn't intersect object
*** Parallel Processing
- Ray tracing is a trivially parallel algorithm
  - Cast rays in parallel
  - Cast reflection, refraction, shadow rays in parallel
  - Calculate ray intersection in parallel independently
** Pros and Cons
*** Advantages
- All the advantages of the local illumination model
- Also handles shadows, reflection, and refraction
*** Disadvantages
- Computationally expensive
- No diffuse inter-reflection between surfaces (i.e. color bleeding)
- Not physically accurate
- *Other techniques exist to handle these shortcomings, at even greater
  expense!*
* Environment Mapping
- An alternative way for modeling global reflections
- How to create this effect
- An efficient technique for approximating the appearance of a *reflective
  surface* by means of a *pre-computed texture image*
- Cheap attempt at modeling reflections
- Make surfaces look metallic
- The poor person's ray tracing method (only consider reflection ray)
- Use six textures to model faces of a cube
- Assume cube faces infinitely far away
- The normal (or reflected eye vector) is used to find which of the textures and
  what texture coordinate
  - Texture is transferred in the direction of the reflected ray $r$ from the
    environment map onto the object
  - reflected ray: $r=2(n \cdot v)n-v$
  - What is in the map?
    - Store colors of every possible direction in pre-computed texture maps
    - Lookup texture maps based on reflected vector
** Represent the Map
*** Cubic Mapping
- The map resides on the surfaces of a cube around the object
  - Typically, align the faces of the cube with the coordinate axes
- To generate the map
  - For each face of the cube, render the world from the center of the object
    with the cube face as the image plane
    - Rendering can be arbitrarily complex (it's off-line)
- To use the map
  - Index the R ray into the correct cube face
  - Compute texture coordinate
*** Sphere Mapping
- Map lives on a sphere
- To generate the map:
  - Render a spherical panorama from designed center point
- To use the map
  - Use the orientation of the R ray to index directly into the sphere
** Limitations
- Assume distant environment
- Only works for convex objects (i.e. no self inter-reflections)
* Radiosity
- Handles the shortcomings of ray-tracing /only/ for diffuse surfaces
- *Radiosity* The /radiant (luminous) exitance/ is the radiant flux/power per
  unit area leaving a surface
- Model light effects by considering the physical lays governing the radiant
  energy transfer
- The radiosity model computes radiant-energy interactions between all the
  surfaces in a scene
** Key Idea #1
- Radiance independent of direction
- Surface looks the same from any viewpoint
- No specular reflections
*** Diffuse surface
- Diffuse emitter
  - Transform x to some constant over the angle
- Diffuse reflector
  - Reflectivity constant
** Key Idea #2
- Radiosity solution is an approximation, due to discretization of scene into patches
- Subdivide scene into small polygons
*** Constant Surface Approximation
- Radiance is constant over a surface
  - Transform x to a constant over x
- surface element is given by its index
** Equation
- Emitted radiosity = self-emitted radiosity + received and reflected radiosity
- $Radiosity_i = Radiosity_{self,i} + \sum_{j=1}^{N}a_{j \rightarrow i}Radiosity_i$
- Radiosity equation for each of the $N$ polygons i
- $N$ equations; $N$ unknown variables
** Algorithm
- *Subdivide the scene into small polygons*
  - Mesh that determines final solution
- *Compute for each polygon a constant illumination (radiosity) value*
  - Transfer of energy between polygons
- *Solve linear system*
  - Results in power (color) per polygon
- *Choose a viewpoint, and display the visible polygons*
  - loop
** Energy conservation equation
- $\phi_i = \phi_{e,i} + \rho_i \sum_{j=1}^N \phi_jF(j \rightarrow i)$
** Compute Form Factors
- The form factor specifies the fraction of the energy leaving one patch and
  arriving at the other. 
- In other words, it is an expression of radiant exchange
  between two surface patches
*** Reciprocity
- $F(i \rightarrow j)A_i=F(j \rightarrow i)A_j$
- For each polygon
*** Form Factors for infinitesimal surfaces
- $F_{j \rightarrow i}$ = the fraction of power emitted by $j$, which is received
  by $i$
- _Area_
  - if $i$ is smaller, it receives less power
- _Orientation_
  - if $i$ faces j, it receives more power
- _Distance_
  - if $i$ is further away, it receives less power
- _Visibility_
  - if not visible, it receives zero power
- $F(j \rightarrow i)=\dfrac{\cos\theta_x\cos\theta_y}{\pi r^2_{xy}}dA_iV(x,y)$
- Double integrate to solve
**** How to compute?
- Closed form
  - Analytical
- Hemicube
*** Nusselt Analog
- Nusselt developed a geometric analog which allows the simple and accurate
  calculation of the form between 
  - a surface and
  - a point on a second surface
- The form factor is, then, the area projected on the based of the hemisphere
  divided by the area of the base of the hemisphere, or (A/B)
**** Hemicube
- Project the patch on hemicube
- Add hemicube cells to compute form factor
- This means we need to calculate the form factor for each cell in order to
  pre-compute using delta form factors
***** Observations
- Depth information per pixel evaluates visibility
  - very expensive
- FFs for all polygons in scene
  - Usually computed when needed
  - Computationally expensive
  - Memory complexity is quadratic
- Hardware rendering (Z-buffer)
- Severe aliasing: Small polygons "disappear"
** Solve Linear System
*** Matrix Conversion
*** Iterative approaches
**** Jacobian (gathering)
- Start with initial guess for energy distribution (light sources)
- Update radiosity/power of all patches based on the previous guess
  - $\phi_i = \phi={e,i} + \rho_i\sum_{j=1}^N\phi_jF(j \rightarrow i)$
- Repeat until converged
** Rendering
- The final $\phi_i$'s can be used in place of intensities in a standard renderer
- Radiosities are constant over the extent of the patch
- A standard renderer requires vertex intensities
- If the radiosities of surrounding patches are known, vertex radiosities can be
  estimated using bilinear interpolation
*** Vertex Intensity: Bilinear Interpolation
- Get radiosity value for each polygon vertex
  - Using bilinear interpolation techniques
- Make radiosity smooth during rendering
** Benefit
- Global illum method: modeling diffuse inter-reflection
- Color bleeding
- Soft shadows
  - an area light source casts a soft shadow from a polygon
- No ambient term hack
  - when you want to look at your object in low light, you don't have to adjust
    parameters of the objects
  - just the intensities of the lights
- View independent
** Limitation
- Radiation is uniform in all directions
- Radiosity is piece-wise constant
  - usual renderings make this assumption, but then interpolate cheaply to fake a
    nice-looking answer
  - introduces quantifiable errors
  - No surface is transparent or translucent
  - Reflectivity is independent of directions to source and destination
* Forward/Inverse Kinematics
** Definition
- *kinematics* Study of movement without the consideration of masses or forces
  that bring about the motion
** Degrees of Freedom (DOFs)
- *DOF* The set of independent displacements that specify an object's pose
  - How many degrees of freedom for a paper airplane?
    - 6 (3 for position, 3 for orientation)
    - x, y, z, roll, pitch, yaw
*** Joints
- *Elbow* 1 DOF
- *Wrist* 2 DOF
- *Shoulder* 3 DOF
*** Configuration vs Work Space
**** Configuration space
- defines the possible object configurations
- *Degrees of freedom*
  - Number of params that are necessary and sufficient to define position in
    configuration
**** Work space
- The space in which the object exists
- *Dimensionality*
  - R^3 for most things, R^2 for planar arms
** Animation Basics
*** Forward Kinematics
- Compute the configuration (pose) given individual DOF values
**** basic method
- An animator can specify the configuration over time
- Computer will find the position of the end effectors
*** Inverse Kinematics
- What if an animator specifies position of end-effectors?
- Compute individual DOF values that result in specified end effectors'
  positions
**** basic method
- Animator specifies the positions of the end effectors
- Computer finds the DOF values
- Challenge: finding a closed form solution is very difficult
**** Why Inverse Kinematics
- Motion capture,
- Basic tools in character animation,
  - key frame generation
  - animation control,
  - interactive manipulation,
- Computer vision (video-based mocap),
- Bioinformatics (Protein Inverse Kinematics),
- Etc.
**** Method
- Given end effector's positions, compute required joint angles
- In simple case, analytic solution exists
  - Use trig, geometry, and algebra to solve
***** Challenge
- Analytical solution only works for a fairly simple structure
- Numerical/iterative solution needed for complex structure
***** Numerical Approaches
- Inverse kinematics can be formulated as an optimization problem
****** Function Optimization
- Finding the minimum for nonlinear functions
  - Find global minimum/maximum
- How to use optimization to solve a linear system
  - Create a function in however many variables in the linear system (squaring
    the original linear functions), and find
    the global minimum/maximum
****** Formulation
- So how to convert the IK process into an optimization function?
- Find the joint angles $\theta$ that minimize the distance between hypothesized
  character position and user specified position
****** Gauss-Newton
1. initialize joint angles with an initial angle
2. Update joint angles (using a step size specified by user) until the solution
   converges
   1. The value is optimal when the gradient is 0
******* When to stop?
1. When the change of solution from prev iteration the current one is below a
   user specified threshold
2. dd
***** Iterative Approaches
- More general and can be applied to more complex models 
- C/C++ Optimization Library
  - levmar: nonlinear least squares algorithms in C/C++
    - works with or without analytical Jacobian matrix
    - Speeds up the optimization process with analytical Jacobian matrix
**** Concerns
- Is the solution unique?
  - Ambiguity of IK
  - Allows multiple solutions
  - Possibly infinite solutions
- Is there always a good solution?
  - Solution may not even exist
  - Generally ill-posed problem when the number of DOFs is higher than the
    number of constraints
***** Additionally objective
- Minimal change from a reference pose $\theta_0$
  - Essentially we make the problem well-defined by looking for the optimal
    solution where the solution is optimal where the pose is minimally changed
    - Satisfy the constraints
    - Then minimize the difference between the solution and the reference pose
- Naturalness $g(\theta)$ (particularly for human characters)
  - Use motion-capture together with machine learning techniques to find a good
    function for this /black-box/ function
**** Summary
- Very simply structure allows an analytic solution
- Most of complex articulated figures requires a numerical solution
- May not always get the "right" solution
  - Need additional objectives
* Keyframe Interpolation and Smooth Curve
** Computer Animation
- Animation
  - Making objects move in virtual world
- Compute animation
  - The production of consecutive images, which, when displayed, convey a
    feeling of motion
*** Topics
- Rigid body simulation
  - Bouncing ball
  - millions of chairs falling down
- Natural phenomenon
  - water, fire, smoke, mud, etc.
- Character animation
  - Articulated motion, e.g. full-body animation
  - Deformation, e.g. face
*** Criterion
- Physically correct
  - Rigid body-simulation
  - Natural phenomenon
- Natural looking
  - Character animation
- Expressive
  - Cartoon animation
** Keyframe Interpolation
- Inverse Kinematics can be used to create key poses
*** Why Keyframe Animation?
- Initial motivation was for 2D animation
- Highly skilled animators draw the key frames
- Less skilled (lower paid) animators draw the in-between frames
- Time consuming process
- Difficult to create physically realistic animation
- *Today* the in-between frames are automatically computed
*** 3D animation
- Animators specify important key frames in 3D
- Computers generate the in-between frames
- Some dynamic motion can be done by computers (hair, clothes, etc)
- Still time consuming (toy story took 4 years)
*** Keyframing
- Specify the keyframes
- Specify the type of interpolation
  - linear, cubic, parametric
- Specify the speed profiles
- Computer computes in-between frames
**** A keyframe
- *2D animation* a keyframe is usually a single image
- *3D animation* each keyframe is defined by a set of parameters
**** Parameters
- Position and orientation
- Body deformation
- Facial features
- Light
- Hair and clothing
** Curve representation and interpolation
*** Functions
**** Linear
- Linearly interpolate the parameters between keyframes
***** Limitations
- If motion is highly nonlinear
- Requires a large number of keyframes
**** Natural cubic
- Can use a cubic function to represent a 1D curve
***** Constraints
- Need 4 keyframes to determine the four coefficients of the cubic function
***** 2D Trajectory Interpolation
- Each point on the trajectory is associated with a time stamp $t$.
- Perform interpolation for each comp separately
- Combine result to obtain parametric curve
***** Issues
- Position of a single keyframe will change the whole shape of the animation
- Does not provide local control of the curve
  - Can manipulate basis matrix to allow local control
**** Hermite cubic
- Determined by
  - Endpoints $P_1$ and $P_2$
  - tangent vectors $R_1$ $R_4$
- Use these vectors to control the curve, i.e. construct control vector
- Given the endpoints and tangent vectors, solve for the basis matrix
**** Bezier cubic
- Indirectly specify tangent vectors by specifying two intermediate points
- The interpolation usually does not go through the intermediate points
***** Basis Matrix
- Establish the relation between Hermite and Bezier control vectors
  - Use the starting points together with the tangent vectors
  - Remember tangent vectors are determined by the starting points and the
    intermediate points
- Plugin the relationship to the Hermite equation to obtain Bezier
  representation
*** Complex curves
- Suppose we want to draw a more complex curve
- Why not use a high-order Bezier?
  - Wiggly curves
  - No local control
- Instead, splice together a curve from individual segments that are cubic
  Beziers
  - Why cubic?
    - Lowest dimension with control for the second derivative
    - Lowest dimension for non-planar polynomial curves
* Spline Curve and Multiple Keyframe Interpolation
** Desirable properties
*** Continuity
- C^0: Points coincide, velocities don't
- C^1: Points and velocities coincide
- C^2: Points, velocities, and acceleration coincide
- Cubic curves are continuous and differentiable
- We only need to worry about the derivatives of two points when they meet
*** Local control
- We'd like our spline to have *local control*
- That is, have each control point affect some well-defined neighborhood around
  that point
*** Interpolation
- Bezier curves are *approximating*
  - The curve does not (necessarily) pass through all the control points
  - Each point pulls the curve toward it, but other points are pulling as well
** Representations
*** B-Splines
- We can join multiple Bezier cures to create B-splines
- Ensure C^2 continuity when two curves meet
**** Continuity
- Suppose we want to join two Bezier curves $V$ and $W$ so that C^2 continuity is
  met at the joint
  - The end point of $V$ should be the same as start point of $W$ (point)
  - In addition first and second derivatives should be the same (velocity and
    acceleration)
  - This will yield three equations
  - From these three you can describe the next segment
**** de Boor points
- Instead of specifying the Bezier control points, let's specify the corners
  of the frames that forms a B-Spline
**** Properties
- [x] Continuity
- [X] Local control
- [ ] Interpolation
*** Catmull-Rom Splines
- We can sacrifice C^2 continuity in order to get interpolation and local control
- Since interpolation and local control are properties, this spline
  representation is popular
- Tangent vector for a control point is determined by the difference between the
  next control point and previous control point
- $\tau$ which controls how sharply the curve bends around control points is
  usually chosen to be 0.5
**** Summary
- C^1 continuity, not C^2
- Does not lie within the convex hull of their control points
**** Properties
- [ ] Continuity
- [X] Local control
- [X] Interpolation
** Speed control
- Time warping function to control speed
  - Mapping from the original time line to the output time line
  - Positive
  - *Monotonic* you cannot reverse the time
*** Methods
- Simplest from is to have constant velocity along the path
  - Slope <1 slow down
  - Slope >1 speed up
- Assume motion slows down at beginning and speeds up at the end
* Animation with Motion Capture
- *Motion Capture* a technique of digitally recording movements for
  - entertainment,
  - sports, and
  - medical applications
** History
*** Eadweard Muybridge (1830-1904)
  - First person to photograph movement sequences
  - Find out whether during a horse's trot, all four hooves were ever off the ground at
    the same time
  - The flying horse
  - Animal locomotion (20k pictures about men, women, children, animals, and birds)
*** Rotoscope
- Allow animators to trace cartoon character over photographed frames of live performances
- Invented by Max Fleischer in 1915
- 2D manual capture
- *Koko the clown* first cartoon to be rotoscoped
- *Snow White and her prince* The human character animation (1937)
** Technologies
- "3D Rotoscoping" measuring 3D positions, orientations, velocities, or
  accelerations automatically
*** Electromagnetic
- Each sensor
  - record 3D position and orientation
  - placed on joints of moving objects
- Full-body motion capture needs at least 15 sensors
- Popular system: [[http://www.ascension-tech.com]]
**** Pros
- Measure 3D positions and orientations
- No occlusion problems
- Can capture multiple subjects simultaneously
**** Cons
- Magnetic perturbations (metal)
- Small capture volume (cannot capture subjects outside the volume)
- Cannot capture deformation (facial expression)
- Hard to capture small bone movement (finger motion)
- Not as accurate as optical mocap systems
*** Electromechanical
- Each sensor measures 3D orientations, including
  - 3D accelerometers,
  - 3D gyros, and
  - 3D magnetometers
- Each sensor placed on joints of moving object
- Full body mocap needs at least 15 sensors
- Popular system: [[http://www.xsens.com]]
**** Pros
- Measure 3D orientations
- No occlusion problems
- Can capture multiple subjects simultaneously
- Large capture volume
- Portable and outdoors capture (e.g. skiing)
**** Cons
- Getting 3D position info is not easy
- The root positions is often measured with ultrasonic position sensors
- Cannot capture deformation (facial expression)
- Hard to capture small bone movements (finger motion)
- Not as accurate as optical mocap systems
*** Fiber optic
*** Optical
- Multiple calibrated cameras (>=8) to digitize different views of performance
- Wears retro-reflective markers
- Accurately measures 3D positions of markers
- Popular system: [[http://vicon.com]]
**** Pros
- measures 3D positions and orientations
- The most accurate capture method
- Very high framerate
- Can cap very detailed motion (body, finger, facial, deformation, etc)
**** Cons
- Has occlusion problems
- Hard to capture interactions among multiple actors
- Limited capture volume
- Expensive
*** Video-based
** Pipeline
** Data format
