* CSCE 441-500 
Notes for Computer Graphics
* Course info
- Website [[http://faculty.cs.tamu.edu/jchai/csce441_2016spring]]
- Lecture is more important than the textbook
** Staff
- Dr. Jinxian Chai HRBB 527D (MW 1:30-2:30pm) also by appointment
- Grader: Chaitanya Visveswara (chaitu236@tamu.edu)
- TAs Jianjie Zhang & Fuhao Shi HRBB 505 (jjzhang10@tamu.edu)
- Send emails to both TA and Prof
** Submissions
- CSNet
- Due midnight on day specified
- Source and Win32 executable
- Comment your unreadable code
- No team coding
** Grading
- 60% Assignments
- 15% Midterm
- 25% Final
** Applications
*** Games
*** Movies
*** Visualization
*** Industrial Design
-* Virtual cars,
-* Environments,
-* Aircraft, etc
*** Human computer interactions
-* Microsoft's Kinect for Xbox 360 and Xbox One
*** Realworld Modeling
-* Google Earth
-* Build a model, then render an image from a particular (perhaps different) viewpoint
*** 3D Printing
-* Design 3D object in virtual environment, then print it
*** Visual Data Processing
- Image and Video processing
- Manipulating objects in a virtual space
  - Compose images and models
  - Synthesis the missing background
  - Transform pictures
** Expect to learn
Using OpenGL in programming assignments
*** 2D Graphics
-* Drawing lines, polygons
-* Image processing
*** 3D Graphics
- Transformations
- Lighting
- Ray Tracing (popular in global lighting techniques)
- Geometric Modeling
- Splines
- Animation
** Expected to know
*** Programming Experience
A*ssignments are in C/C++
*** Simple Mathematics
** How much math?
*** General geometry/linear algebra
*** Matrices
-* Multiplication
-* Inversion
-* Determinant
*** Vectors
- Dot product,
- Cross product, linear independence
** Linear Algebra Test
- See todo for deadline
- [[http://projects.cs.tamu.edu/keyser/LAtest/]]
- Need 90% correct by deadline, or will get a 0 for the class
* Into to OpenGL
** What?
*** Computer graphics API
- Developed by SGI in 1992
- Efficient, streaming interface
- 250+ function calls for drawing 2D and 3D graphics
- Hardware independent
- OS independent
- Competes against direct3D from microsoft
  - OpenGL seems to be more popular because it is OS independent
** What it is not
- No commands for windowing
- No commands for obtaining user input
- No commands for anything except drawing on the screen
** Command Formats
- Prefix (e.g. gl)
- Initial capital letters
- Number of components
  - 2: (x, y)
  - 3: (x, y, z)
  - 4: (x, y, z, w)
- Data type
- Vector
** Geogetric Primitives
See documentation for better descriptions
- GL_POINTS
- GL_TRIANGLE_STRIP
- GL_TRIANGLES
  - Each triad of vertices defines a triangle
- GL_LINES
  - Each pair of vertices constitutes a line
- GL_LINE_STRIP
  - Connect each vertex to the previous one
- GL_TRIANBLE_FAN
- GL_LINE_LOOP
  - Like STRIP except the first and last points are connected
- GL_QUADS
- GL_QUAD_STRIP
- GL_POLYGON
* Related Libraries
** GLU (OpenGL Utility Library)
- Part of OpenGL
- Provides higher-level drawing routines such as
  - Spheres,
  - NURBS,
  - Tesselators,
  - Quadric shapes, etc.
** GLUT (GLU Toolkit)
- perform system level IO with host os
- cross platform
- protable window API
* Scan Conversion of Lines
** Line-drawing using "Paint"
** How can we represent and display images
*** Image representation
**** An image is a 2D rectilinear array of pixels
- width*height array where each entry of the array stores a single pixel
- each pixel stores color information
*** Displays
  - LCD
- Oculus
- Smartphone
**** Pixels
- Smallest element of picture
- Integer position (i, j)
- Color information (r, g, b)
***** Luminance pixels
- Gray-scale images (intensity images)
- 0-1.0 or 0-255
- 8 bits per pixel (bpp)
***** Red, green, pixels (RGB)
- Color images
- Each channel: 0-1.0 or 0-255
- 24 bpp
** Digital Scan Conversion
- Convert graphics output primitives into a set of pixel values for storage in
  the frame buffer
- Store image info in the frame buffer
- Display on screen based on what is stored in frame buffer
** How to draw a line
*** Problem
- Given two points (P, Q) on the screen (with int coordinates) determine which
  pixels should be drawn to display a unit width line
**** Idea is to interpolate pixel values through rounding based on a perfect straight-through line (DDA)
***** Special lines
- Horizontal (y is constant)
- Vertical (x is constant)
- Diagonal (increment both)
***** Arbitrary lines
****** Slope-intercept equation for a line
$y = mx + b$
****** Focus on line with slope between 0 to 1
- Manipulate line equation to satisfy this property
*** Digital DIfferential Analyzer (DDA)
- Sample unit intervals in one coordinate
  - e.g. inc x by 1, inc y by m
- Find nearest pixel for each sampled point
**** Limitations
- Floating point operations (slow)
- Rounding (slow)
- Can we do better?
*** Midpoint algorithm
- Next pixel to draw is always East or Northeast from the current pixel
- Given a point, determinte whether the next pixel is E or NE
- Is the line above or below the midpoint (x + 1, y + 1/2)
  - Below: move E
  - Above: move NE
**** Implementation issues
***** How to eval if the midpoint is above or below the line
- properties
  - $y=mx+b (x,y)$ on the line
  - $y<mx+b (x,y)$ below the line
  - $y>mx+b (x,y)$ above the line
- Manipulate $f(x,y) = y-mx-b$ to achieve integers $c,d,e$
  - $f(x,y)=cx+dy+e$
  - if $f(x,y)<0$, then the point is below the line
  - similarly for > and =
***** How to incrementally eval this
- Incremental update to speed up the algorithm
- Fancy discrete math proofs and manipulations
- Need value of $f(x + 1, y + 1/2)$ to determine E or NE
- Build incremental algorithm
- Assume we have a value of $f(x+1, y+1/2)$
  - Find value of $f(x+2, y+1/2)$ if E is chosen
  - Find value of $f(x+2, y+3/2)$ if NE is chosen
***** How to avoid floating point ops
- Just a bunch of plugging into f(x,y), expanding, and simplifying
**** Advantages 
- Only integer operations
*** Midpoint Algorithm: Circle
- Similar idea: but the possible directions seem to change
** Required reqdings
HB 6.1-6.8 (remember ppt more important)
* Scan Conversion of Polygons
** Drawing General Polygons
*** Drawing Rectangles
**** How to avoid overlap?
- *RULE* In this class do *not* draw pixels on the *top* and *right* of the
  rectangle
- Benefit: we never draw the same pixel twice
- There are some limitations
*** How to draw every interior pixel?
- process the scan line from the bottom of the boundaries to the top
**** draw every interior pixel for each scan line
- including pixels intersecting the boundary
- you only draw pixels at "odd" intervals
  - The first interval is defined by the first and second intersection point
  - Next interval is defined by the second and third intersection point etc.
**** Once again follow the rule for avoiding overlapping polygons
**** Overview
***** Intersect scan lines with edges 
- use *coherence* to speed up
  - using the relationship between two intersecting scan lines and the polygon
    edge
***** Find ranges along x
***** Fill interior of those ranges
***** Draw odd intervals only
*** How to store polygon edges?
**** Sorted Edge Tables
- Associate polygon edges with scan lines
- Find out the low end point for each and correlate it with a corresponding
  scanline
- Store the multiple edges associated with each scanline as a linked list
- Exclude horizontal edges
*** Represent edges intersecting current scanline
**** Active Edge List
- List of all edges intersecting current scan-line sorted by their x-values
- Each edge is stored with the following information
| Edge     | Description                              |
|----------+------------------------------------------|
| maxY     | highest y-value                          |
| currentX | x-value of end-point with lowest y-value |
| xIncr    | 1 / slope                                |

*** Algorithm
#+BEGIN_SRC
line = 0
While (line < height)
  Add edges to active edge list from sorted edge table starting at line
    Table sfarting at line
  Remove edges with a maxY equal to currentX
  Fill interior pixels // draw pixels the curreent scan line
  Increment x-values on edges in Active Edge list
  Increment line
#+END_SRC
** Drawing Curved Objects
Neither mid-term or final will test on this!
*** How to draw every interior pixel?
- process the scan line from the bottom to top
- find the intersection positions between the current scan line and the
  boundaries of fill region
- use coherence properties to reduce the process
- fill interior pixel in the odd intervals
** Methods for drawing polygons or curved objects
*** Scanline conversion of polygons
*** Boundary fill (boundary is one color)
- Start with drawn boundaries and an interior point
- Recursively recolor outward from that point
  - If neighbor is different, then recolor and recur
- Everyting within the boundary is changed to that color
*** flood fill (boundary may be different colors)
- Start with a point
- Define color under that point sas the interior color
- Recursively recolor outward from that poing
  - If neighbor is interior color, then recolor and recur
- Contiguous regions are re-colored
*** Boundary vs Flood-fill
- Both start with an interior pixel
- Boundary-fill requires annotating boundary pixels while flood-fill requires
  annotating interior pixels
- Both are appealing to fill in the irregular boundaries
** Reading
- 6-10 and 10-14
* Clipping lines
** Why Clip?
- Do not want to waste time drawing objects that are outside of the viewing
  window (or clipping window)
** Clipping points
- Given a point (x, y) and clipping window (xmin, ymin), (xmax, ymax)
- Determine if the point should be drawn
** Clipping lines
- Given a line with end-points (x0, y0), (x1, y1) and clipping window (xmin,
  ymin), (xmax, ymax)
- Determine
  - whether line should be drawn
  - clipped end-points of line to draw
** Simple Alg
- If both end points inside rect, draw line
- If one end-point outside,
- Intersect line with all edges of rectangle
  - clip the line segment outside the rect, and repeat test with rest of edges
*** Intersecting two lines
- parametric representation of the lines
  - t = 0 corresponds to (x0,y0)
  - t = 1 correspons to (x1, y1)
- Have a system of parametric equations to determine if there is an
  intersection and also to calculate the point of intersection
*** Disadvantages
- Lots of intersection tests makes alg expensive
- Complicated tests to determine if intersecting rectangle
- Is there a better way?
**** Trivial accepts
- big optimization
- How can we quickly decide whether the line segment is entirely inside window
- Answer: test both endpoints
**** Trivial rejects
- How to know if a line is outside of the window
- Answer: both endpoints on wrong side of the same edge, can trivially reject
  the line
** Cohen-Sutherland
*** Classify p0, p1 using region codes c0, c1
- Every end point is assigned to a four-digit binary value, i.e. region code
- Each bit position indicates whether the point is inside or outside of a
  specific window edges
| bit 4 | bit 3  | bit 2 | bit 1 |
|-------+--------+-------+-------|
| top   | bottom | right | left  |

| 1001 | 1000 | 1010 |
| 0001 | 0000 | 0010 |
| 0101 | 0100 | 0110 |
**** If c0 ^ c1 not 0
- bitwise and 0010 ^ 1011 = 0010
- trivially reject
**** If c0 bitwise or c1 not 0
- trivially accept because both points are not on wrong side for any edge
- bitwise or 0010 op 1011 = 1011
**** Otherwise reduce to trivial cases by splitting into two segments
*** Window intersection
- Similar to simple alg
*** Extends easiy to 3D line clipping
- 27 regions
- 6 bits
*** Summarize
- Use region codes to quickly eliminate/incude lines
  - *best algorithm when trivial accepts/rejects are common*
- Must compute viewing window clipping of remaining lines
- More efficient algs exist
  - non-trivial clipping cost
** Liang-Barsky
*** Parametric definition of a line
- x = x1 udeltax
- y = y1 udeltay
- deltax = (x2-x1)
- deltay = (y2-y1), 0<=u<=1
*** Lines are oriented: classify lines as moving inside to out or outside to in
- For lines stating outside, update its starting point
- For lines starting inside, update end point
- For lines paralleling the boundaries and outside, reject it
*** Goal find range of u for which x and y both inside the viewing window
*** Advantages
- Faster than cohen-sutherland
- Extension to 3D is easy
  - parametric rep for 3D lines
  - Compute u1, u2 based on the intersection between ine and plane
** More Complex clipping windows
- The clipping could be any polygon
- Both Cohen and Liang can be extended to any polygon as well curve clipping
** Required reqdings: HB 8-5, 8-6, 8-7, and 8-9
* Clipping polygons
** Why is clippping hard?
- Consider the result of clipping a triangle
  - May be a triangle
  - Or some other polygon such as a 7-gon (7 is the max for a triangle)
- Tough cases such as concave polygons
** Sutherland-Hodgman Clipping
- *convex polygons*
*** Idea
- Apply line clipping to each edge of polygon
- But we may get unconnected lines which is undesirable
- *Basic idea* similar to line clipping
  - Clip against each edge of the window
  - This way the clipped polygon would be connected along the window edges
*** Input/Output for algorithm
- input: list of polygon vertices *in order*
- output: list of clipped polygon vertices consisting of old vertices (maybe)
  and new vertices (maybe)
*** How to clip against window edge
- Go around polygon one vertex (i.e. polygon edge) at a time
- Current vertex has position E
- Previous vertex had position S, and it has been added to the output if appropriate
- *Need to determine output vertices for each polygon lines*
**** Four cases
- both inside S to E
  - include E
- outside to inside S to E
  - include E plus intersection
- inside to outside S to E
  - include intersection
- both outside S to E
  - exclude both
*** Dealing with non-convex polygons
1. split concave polygon into two or more convex polygons (see sec 4-7)
2. use more general polygon clipper
3. perform the algorithm plus post-processing
*** Dealing with more general clipping boundaries
- Do polygon clipping against each boundary edge
*** Summary
- Works for convex input polygons
- Works for convex clipping boundaries
- Easy to pipeline for parallel processing
- Polygon from one boundary does not have to be completed before next boundary starts
** Weiler-Atherton 
- *general polygons*
*** Idea
- trace around perimeter of the fill polygon
- search for the borders that enclose a clipped fill region
*** Procedure
1. process the edges of the polygon fill area in a CCW order until an
   inside-outside pair of vertices is encountered
2. Follow the window boundaries in a CCW direction from the exit-intersection point to
   another intersection point with the polygon
   - *previously processed?*
   - if yes: go to next step
   - if no: continue processing polygon until a previously encountered point is
     encountered
3. Form the vertex list for this section of the clipped area
4. Return to exit-intersection point and continue processing polygon edges in a
   CCW order until another inside-outside pair of vertices is encountered
*** Summary
- Works for general input polygons (concave and convex)
- Handles a clipping window with any polygon shape (concave and convex)
- Can be extended to 3D
- Not as efficient as Sutherland-Hodgman
- Not as efficient as Sutherland-Hodgman
- Not easy to parallelize
** Nonlinear clipping-window boundaries
- Approximate the boundaries with straight-line sections
- Use the existing polygon clipping algorithms for clipping against a general
  polygon-shaped clipping window
** Required readings
HB 8-8
* 2D Transformations
** Point Representation
- We can use a column vector (a 2x1 matrix) to represent a 2D point (x, y).
- A general form of /linear/ transformation can be written as
\[
x' = ax + by + c
\]
or
\[
y' = dx + ey + f
\]
- Transformations can be written as a matrix, vector multiplication.
** Translate
- Re-position a point along a straight line
- Given a point (x, y), and the translation distance (tx, ty).
| x' |   | 1 | 0 | tx |   | x |
| y' | = | 0 | 1 | ty | * | y |
| 1  |   | 0 | 0 | 1  |   | 1 |
- How to translate an object with multiple vertices?
  - Translate each vertex individually
** Rotate
*** About the origin
- Default rotation center: Origin (0, 0)
- For theta
  - >0: Rotate counter clockwise
  - <0: Rotate clockwise
- Rotate (x, y) /about the origin/ by theta
  - Result: (x', y')
- Use trig to calculate the angle to rotate (x, y) to get (x', y')
- Matrix form
| x' |   | cos theta | -sin theta | 0 |   | x |
| y' | = | sin theta | cos theta  | 0 | * | y |
| 1  |   | 0         | 0          | 1 |   | 1 |
*** About any point 
- Idea
  - Translate the rotation center to the origin
  - Perform the rotation
  - Translate it back
| x' |   | 1 | 0 | px |   | cos theta | -sin theta | 0 |   | 1 | 0 | -px |   | x |
| y' | = | 0 | 1 | py | * | sin theta | cos theta  | 0 | * | 0 | 1 | -py | * | y |
| 1  |   | 0 | 0 | 1  |   | 0         | 0          | 1 |   | 0 | 0 | 1   |   | 1 |
** Scale
- Alter the size of an object by a scaling factor (Sx, Sy)
- Apply scaling to each vertex
- For now, translation will also occur
  - Consider scaling without translation
| x' |   | Sx |  0 | 0 |   | x |
| y' | = |  0 | Sy | 0 | * | y |
| 1  |   |  0 |  0 | 1 |   | 1 |
*** Without translation
| x' |   | 1 | 0 | px |   | Sx |  0 | 0 |   | 1 | 0 | -px |   | x |
| y' | = | 0 | 1 | py | * |  0 | Sy | 0 | * | 0 | 1 | -py | * | y |
| 1  |   | 0 | 0 | 1  |   |  0 |  0 | 1 |   | 0 | 0 | 1   |   | 1 |
** Shearing
- in x
| x' |   | 1 | h | 0 |   | x |
| y' | = | 0 | 1 | 0 | * | y |
| 1  |   | 0 | 0 | 1 |   | 1 |
- in y
| x' |   | 1 | 0 | 0 |   | x |
| y' | = | g | 1 | 0 | * | y |
| 1  |   | 0 | 0 | 1 |   | 1 |
*** Interesting facts
- A 2d rot is three shears
- Shearing will not change the area of the object
- Any 2d shearing can be done by a rotation, followed by a scaling, follow by a rotation.
** Reflection
- About X-axis
| x' |   | 1 |  0 | 0 |   | x |
| y' | = | 0 | -1 | 0 | * | y |
| 1  |   | 0 |  0 | 1 |   | 1 |
- About Y-axis
| x' |   | -1 | 0 | 0 |   | x |
| y' | = |  0 | 1 | 0 | * | y |
| 1  |   |  0 | 0 | 1 |   | 1 |
- About origin
| x' |   | -1 |  0 | 0 |   | x |
| y' | = |  0 | -1 | 0 | * | y |
| 1  |   |  0 |  0 | 1 |   | 1 |
*** About an arbitrary line
Idea, rotate, reflect, rotate back (similar to above arbitrary methods)
** Affine Transformation
- Translation, scaling, rotation, shearing are all affine transformations
- Affine transformation - transformed point is a linear combination of the
  original points
- Essentially using basic transformations to obtain a composite matrix to
  describe a complex transformation
*** How to find affine transformations
- How many points needed to estimate affine transformation?
- Three because you have two equations for each correspondent
- 6 unknowns
| x' |   | m11 | m12 | m13 |   | x |
| y' | = | m21 | m22 | m23 | * | y |
| 1  |   | 0   | 0   | 1   |   | 1 |
** Composing transformation
- Composing transformation - the process of applying several transformations in succession to
  form done overall trans
- The arbitrary methods above, can use pre-multiplication to get a composite
  transformation matrix
*** Order is important
- Matrix multiplication is associative
- Transformation products may not be commutative!
- Example: rotation and translation are not commutative
**** Some cases where it does not matter
- translation
- scaling
- rotation
** Why use 3x3 Matrices?
- So that we can use matrix, vector multiplication for all transformations
- This allows us to pre-multiply all the matrices together
- The point (x, y) needs to be represented as (x,y,1)
  - This is call *homogeneous coordinates*
  - How to represent a vector (v_x, v_y)?
    - (v_x, v_y, 0)
  - Remember,
    - for *point* the homogeneous coordinate is 1
    - for *vector* it is 0
** Applications
*** Animation
*** Image/object manipulation
*** Viewing transformation
* 3D Transformations
- A 3D point (x,y,z) - x,y, and z coordinates
- we will still use column vectors to represent points
- Homogeneous coordinates of a 3D point (x,y,z,1)
- Transformation will be performed using a 4x4 matrix
** Right-handed Coordinate System
$x*y=z$; $y*z=x$; $z*x=y$
** Translation
** Rotation
- 3D rotation is done around a rotation *axis*
- Fundamental rotations - rotate about x, y, or z axes
- CCW rotation is referred to as positive rotation (when you look down negative axis)
- Keep the axis of rotation constant
  - Replacement
  - I.e. treat the other two axes as if they are x and y axes in 2d rotation
*** Arbitrary axis
- Set up a transformation that superimposes rotation axis onto one coordinate axis
- Rotate about the coordinate axis
- Translate and rotate object back via inverse of the initial transformation
- The resulting composite matrix is *orthonormal*
  - column rows linearly independent *orthogonal*
  - column rows are unit vectors *normalized*
  - inverse of the matrix is its transpose
** Scaling
- Very similar to 2d transformation
** Inverse of 3D transformations
- Invert the transformation matrix
* Coordinate transformation
** Review
- Dot product: angle between two vectors
- Cross product: area determined by two vectors
** 2D Cartesian coordinate system
- Axes described by unit vectors i and j.
  - $i \cdot i = 1$
  - $j \cdot j = 1$
  - $i \cdot j = 0$
- Any 2D vector can be represented as xi + yj
- Any 2D vector starting from the origin can be described as $op = xi + yj$
*** Transform object description


- from $i'j'$ to $ij$
- Use composite matrix which performs a possible rotation and a possible
  translatio
- Build a relation using three vectors
  - from old to new origin
  - from new origin to point
  - from old origin to point
- new point given by
  - $[i' j' o']p$
*** Alternative way to look at the system
- transforming the old coordinate system to the new coordinate system
- then take the inverse transformation coordinates to achieve the old
  coordinates in the new coordinate system
** 3D Coordinate Transformation
- Once again use 4 by 4 transformation matrix to model the transformation
** Composite 2D Transformation
- Describe the model transformations in 2d objects
- Multiple coordinate transformations to model animation of a character
- *Forward kinematics function*, mapping of a local point of a character to a global
  coordinate system
*** Animate the character
- *keyframe animation*
  - manually pose the character by choosing appropriate position and angle parameters
  - linearly interpolate in between poses
  - works for any types of articulated characters
** Composite 3D Transformation
- Similarly, we can extend composition transformation from 2D to 3D
- Once again a 4x4 transformation matrix
* Hierarchical models
** Symbols and Instances
- Mose graphics API supports a few primitives:
  - sphere
  - cube
  - cylinders
- These symbols are instanced using instance/model transformation
** Instance translation
- created by modifying the model-view matrix
#+BEGIN_SRC
glMatrixMode(GL_MODELVIEW)
glLoadIdentity(...)
glTranslate(...)
glRotate(...)
glScale(...)
house()
#+END_SRC
*** Opengl implementation
- Opengl postmultiplies transformation matrices as they are called
- Each subsequent transformation call concatenates the designated transformation
  matrix on the right of the composite matrix
We must invoke the transformation in the opposite order from which they are applied
*** Consider a car model
- 2 primitives: chassis + wheel
- 5 instances: 1 chassis + 4 wheel
- Represent the car as a tree to show relationship between the parts
** Hierarchical Modeling
- Hierarchical model can be composed of instances using trees or directed
  acyclic graphs (DAGs)
- Edges contains geometric transformations
- Nodes contains geometry
- Drawing is done most efficiently using a DFS
** Articulated Models
- You can draw these models as long as
  - you know how to draw each primitive in local reference frames
  - you know how to call transformation matrices to model the relationship
    between the primitives
* 3-D Viewing
** 3D Geometry Pipeline
- Object space
- World space
- View space
- Normalized projection space
- Screen/Image space
*** OpenGL Codes
- Just as in hierarchical modeling
- Apply transformations in reverse order
- That is
  - viewport transformation
  - projection transformation
  - viewing transformation
** Rotate and translate camera to desired camera viewpoint
*** Camera coordinate
**** Canonical coordinate system
- usually right handed (looking down z axis)
- convenient for project and clipping
**** Mapping from world to eye coordinates
- eye position maps to origin
- right vector maps to x axis
- up vector maps to y axis
- back vector maps to z axis
- Then you use this mapping to use 3D coordinate translation to find the mapping
  from world to eye coordinates
*** Viewing transformation
- We have the camera in world coordinates
- We want to model translation T which takes object from world to camera
- Trick: find inverse of T taking object from camera to world
**** gluLookAt
- Need to know a few things
- Camera center (eye)
- Point on an object (of interest) to look at
- Up vector for the camera coordinate system system
***** How to properly configure up, right, and back vectors
- make sure that the right vector will be perpendicular to the up vector
- accomplish this by taking the cross product of the vector from eye origin to
  the point to look at and the up vector.
*** Projection
- General definition
  - transform points in n-space to m-space (m<n)
- In computer graphics
  - map 3D coordinates to 2D screen coordinates
**** Map 3D coordinates to 2D coordinates
***** Perspective projection
- maps points onto "view plane" along projectors emanating from "center of
- consider the projection of a 3D point on the camera plane
  - using similar triangles
  - we can compute the scale for the x and y coordinates
- transformation
  - $(x,y,z) \left arrow (-dx/z, -dy/z)$
  - remember to use homogeneous coordinates in order to turn this transformation
    into a linear transformation
***** Perspective effects
- Distant object object becomes small
- The distortion of items when viewed at an angle (spatial foreshortening)
***** Properties of Perspective Projection
****** Perspective projection is an example of projective transformation
- lines maps to lines
- parallel lines do not necessarily remain parallel
  - *vanishing points* each set of parallel points not parallel to the
    projection plane will vanish at their intersection point in the projection
- ratios are not preserved
******* Advantage: size varies inversely proportionally to distance to look more realistic
***** Parallel Projection
- Center of projection is at infinity
****** Orthographic projection
- Special case of parallel projection
- Direction of projection (DOP) perpendicular to view plane
- Depth values are all mapped to 0
****** Properties
- Not realistic looking
- Good for exact measure
- Are actually affine trasformation
  - parallel lines remain parallel
  - ratios are preserved
  - angles are often not preserved
- Often used in CAD programs
**** Perspective Projection Volume
- The center of the projection and the portion of projection plane that map to
  the final image form an infinite pyramid. The sides of pyramid called clipping
  planes
- Additional clipping planes are inserted to restrict the range of depths
  - Far clip plane
  - Near clip plane
  - Both planes will define a viewing (truncated) pyramid
    - Objects not inside the pyramid will be clipped
- The truncated pyramid maps to a cube in the normalized projection space (left-handed)
- The view space (right-handed)
***** OpenGL
- Truncated pyramid has 6 sides, defined using ~glFrustum~ function which takes
  6 parameters
  - First four parameters define the polygon dimensions for the near clipping plane
  - The second two parameters define the distances of both clipping planes from
    the eye
- ~gluPerspective~ is the other function you can use
  - Essentially calls glFrustum creating a symmetric truncated pyramid
- Obtain normalized depth values between -1 and 1
*** Viewport transformation
- In OpenGL, use ~glViewport~ function
  - specify a rectangle on screen for the viewport
  - depth values are transformed to be between 0 and 1
* Hidden Surface Removal
** Rendering Pipeline
- Modeling transformation
- Lighting
- Viewing transformation
- Project transformation
- Clipping
- Scan conversion
- Image
** Hidden Surface Removal
*** Hidden Surfaces
- Motivation: determine which pixels are visible or not visible from a certain viewpoint
**** Polygon mesh representation
- Vertex and face list
  - vertex list maps faces that contain the vertex
  - face list maps vertices that form a face to that face
*** Algorithms
**** Backface Culling
- *idea* cull triangles which are not facing the camera
***** Advantages
- Speeds up rendering by removing roughly half of polygons from scan conversion
***** Disadvantages
- Assumes closed surface with consistently oriented polygons
- Not a true hidden surface algorithm
**** Painter's Algorithm
- *Idea* similar to oil painting
  - draw background first
  - then most distant object
  - then nearer object
  - and so forth
- Sort polygons according to distance from viewer
- Draw from back (farthest) to front (nearest)
  - the entire object
- Near objects will overwrite farther ones
- Problem: objects can have a /range/ of depth, not just a single value
- Need to make sure they don't overlap for this algorithm to work
  - Might need to split up some polygons
***** Advantages
- Simple algorithm for ordering polygons
***** Disadvantages
- Splitting is not an easy task
- Sorting can also be expensive
- Redraws same pixel many times
**** Binary Space Partitioning Trees
- Basic principle: Objects in the half space opposite of the viewpoint do not
  obscure objects in the half space containing the viewpoint; thus, one can
  safely render them without covering foreground objects
***** BSP Tree
- Organize all of space (hence partition) into a binary tree
- /pre-process/ overlay a binary tree on objects in the scene
- /run-time/ correctly traversing this tree enumerates objects from back to front
  similarly to painters' algorithm
- *Idea* divide space recursively into half spaces by choosing splitting planes
  - splitting planes can be arbitrarily oriented
****** Details
- positive half-space objects are place in the left branch
- push down objects into the appropriate child branch as you go down until every
  leaf has only 1 object
- objects which are split by a half-space-line can be split into two objects
****** Summary
- Split along plane containing any polygon
- classify all polygons into positive or negative half-space of the plane
  - if a polygon intersects a plane, split it into two
- recurse down positive half-space
- recurse down negative half-space
***** Building a BSP tree for polygons
- Choose a splitting polygon
- Sort all other polygons as
  - front
  - behind
  - crossing
  - on
- Add "front" polygons to front child, "behind" to back child
- Split "crossing" polygons with infinite plane
- Add "on" polygons to root/current node
- Recur
****** Drawing:
- Basically in-order traversal
- But it depends on camera view point
- If eye is in front of plane
  - Draw "back" polygons
  - Draw "on" polygons
  - Draw "front" polygons
- If eye is behind plane
  - Draw "front" polygons
  - Draw "on" polygons
  - Draw "back" polygons
- If eye is on plane
  - Draw "front" polygons
  - Draw "back" polygons
***** Speed improvement
- Take advantage of view direction to cull away polygons behind viewer
***** Summary
****** Pros
- Simple, elegant scheme
- no depth comparisons needed
- polygons split and ordered automatically
- works for moving cameras
- only writes to frame buffer (similar to painters algorithm)
****** Cons
- Computationally intense pre-process state restricts algorithm to static scenes
- Worst-case time to construct tree: O(n^3)
- Splitting increases polygon count
  - Again O(n^3) worst case
- Redraws same pixel many times
- Choosing splitting plane not an exact science
- Not suitable for moving objects
**** Z-Buffer
***** Main idea
- Simple modification to scan-conversion
- Maintain a separate buffer storing the closest "z" value for each pixel: depth buffer
- only draw pixel if depth value is closer than stored "z" value
  - Update buffer with closest depth value
  - work in normalized coordinate space [0.0...1.0]
***** Algorithm
- Initialize the depth buffer and frame buffer for every pixel
- Process each polygon in a scene, one at a time
***** Calculate "z"
- Easy to implement for polygon surfaces using plane equation
- How can we speed up the calculation?
  - use incremental update during scan-line conversion
  - update "z" values using scan line conversion algorithm
***** Pros
- Always works. nearest object always determines the color of a pixel
- polygon drawn in any order
- commonly in hardware
***** Cons
- Needs a whole extra buffer (depth buffer)
- Requires extra storage space
- Still overdraw
***** Opengl
- In opengl, depth values are normalized to [0.0, .1.0]
- Specify initial depth-buffer value
- Activate depth-testing operations
**** Ray casting
* Color
** Human Vision
- Color/light is electromagnetic radiation within a narrow frequency band
*** Components
- Incoming light
- Human eye
** Visible Light
- Human eye can see "visible" light in frequency between 380nm-780nm (Spectral color)
- Visible light is a combination
** Spectral Energy Distribution
- A light source emits all frequencies within the visible range to produce white light
- Three different types of lights
*** Perception of Object colors
- Why does the object appear different color under different light?
**** Perceived object color depends on
- Incoming color of light
- Surface reflectance property
*** Different types of light
Daylight
Fluorescent
Incandescent
*** Ideal Color Representation
- *Unique* one-to-one mapping
- *Compact* require minimal number of bits
- *General* represent all visible light
- *Perceptually appropriate* tell us hue, luminance, purity of color
** Hue, Brightness, & Purity
- *Hue* the color of light corresponding to the dominant frequency of the color
- *Brightness* corresponds to the total light energy and can be quantified as
  the luminance of the light
- *Purity (or saturation)* describes how close a color appears to be a pure
  spectral color, such as red
** Color Representation
*** Human Vision
**** Photo-receptor cells in the retina:
***** Rods
- 120 million rods in retna
- 1000X more light sensitive than cones
- Discriminate between brightness in low illumination
- Short wave-length sensitive
***** Cones
- 6-7 million cones
- Responsible for high-resolution vision
- Discriminate colors
- Three types of color sensors (64% red, 32% green, 2% blue)
- Sensitive to any combination of three colors
*** Tristimulus of Color Theory
- Spectral-response of each of the three types of cones
- Color matching function based on RGB
  - Any spectral color can be represented as a linear combination of these
    primary colors
**** Color is psychological
- Representing color as a linear combination of red, green, and blue is related
  to cones, not physics
- Most people have the same cones, the there are some people who don't - the sky
  might not look blue to them (although they will still call it blue nonetheless)
**** Additive and Subtractive color
- Normalized weights: between 0 and 1
  - RGB color model
    - white [1 1 1]^T
    - green [0 1 0]^T
  - CMY color model (Cyan Magenta Yellow)
    - white: [0 0 0]^T
    - green: [1 0 1]^T
**** RGB color space
- Can be viewed as a 3D space
***** RGB cube
- Easy for devices
- Can represent all the colors? No
- But not perceptual
- Where is brightness, hue, and saturation?
**** Summary
- Since 3 different cones, the space of colors is 3D
- We need a way to describe color within this 3D space
- No finite set of light sources can be combined to display all possible colors
- We want something that will let us describe any visible color with *additive*
  combination of three primary (imaginary) colors
*** The CIE XYZ System
- CIE - Commision Internationale de l'Eclairage
  - International Commission on Illumination
  - Sets international standards related to light
- Defined the XYZ color system as an int'l standard in '31
- X, Y, Z are three primary colors
  - imaginary colors
  - all types of color can be represented by an additive combination of the
    three primary colors
  - Standard, but not very intuitive
  - There is more than one way to specify color
  - Variety of color models have developed to help with some specifications
  - Not possible to represent all visible color
**** Color Matching Functions
- Given an input spectrum, we want to find the X, Y, Z coordinates for that color
- Color matching functions tell how to weigh the spectrum
**** XYZ space
- The visible colors form a "cone in XYZ space
- For visible colors, X, Y, Z are all possible
- C_x, C_y, C_z are not visualizable
**** Luminance and Chromaticity
- The intensity *luminance* is just X+Y+Z
  - Scaling X, Y, Z increases intensity
  - We can separate this from the remaining part, *chromaticity*
- Color = Luminance + Chromaticity (Hue and Purity)
- Project the X+Y+Z=1 slice along the Z-axis
- Chromaticity is given by the x, y coordinates
**** Functions of Chromaticity Diagram
- Determining purity and hue (dominant wave length) for a given color
- Identify complementary colors
- Compare color gamuts generated by different primaries (e.g. on different
  devices)
***** White Point
- White: at the center of the diagram
- Approximation of average of daylight
***** Saturation/Purity
- As you move on line from white to spectral color, you increase the saturation
  of that color
- How to compute this?
  - The ratio of the magnitude of the saturation vector by the magnitude of the
    "100% saturated" vector
***** Hue
- Whats the dominant wavelength of color for which the nearest edge is not spectral?
  - Take complementary color of opposite edge
- Whats the purity?
***** Combine two colors
- Two colors, A and B
- Vary the relative intensity
- Generate any color on the line between A and B
***** Complementary colors
- Complementary colors are those that will sum to white
- The distances to white determine the amounts of each needed to produce white
  light
***** Combine three colors
- vary relative intensity
- generate any color in the triangle between them
***** Gamut
- Display devices generally have 3 colors (a few have more)
  - e.g. RGB in monitor
- The display can therefore display any color created from a combination of
  those 3
- Display range that the monitor can produce by combining its colors is called
  that display's *gamut*
- How to to find the appropriate color space to represent all visible colors
  - There is no perfect color representation
*** RGB
- Red, Green, Blue
- Common spec for most monitors
- Not standard
*** CMY
- Cyan, Magenta, Yellow
- Commonly used in printing
- Generally used in a /subtractive/ system
*** CMYK
- CMY, Black
- Comes from printing process - Since CMY combine to form black, can replace
  equal amounts of CMY with Black, saving ink
*** HSV Color Model
- Perceptually appropriate
- Nonlinear transform between the HSV and RGB space
* Image Filtering
- *filter* Process that removes from a signal some unwanted component or feature
- filtering is altering the *range* of image
- warping is altering the *domain* of image
- filtering in spatial domain involves applying a filter function to the input
  image
- *Common themes*
  - Interwindow iteration
  - Keeping pixel weights normalized so they sum to 1
** Gaussian Filtering
- the filter function is a discrete approximation to Gaussian function (with
  sigma equal to 1.0)
- filter applied by modifying surrounding pixels
*** Features
- blurs image
- preserves details only for small sigma
** Median Filtering
- for each neighbor in image sliding the window
- sort pixel values
- set the center pixel to the median
- increasing the size of the window increases blurriness
- straight edges kept
- sharp features lost
*** Features
- can remove outliers
- Window size controls size of structure
- Preserve some details but sharp corners and edges might be lost
- Blurs image
- Removes simple noise
- No details preserved
** Bilateral Filtering
- Affecting or undertaken by two sides equally
- *Property*
  - Convolution filter
  - Smooth image but preserve edges
  - Operates in the domain and range of image
- *Procedure*
  - Apply a Gaussian filter on pixel weights based on value difference
  - Smooth pixel values
*** Comments
- Can work with any reasonable distances function
- Easily extended to higher dimension signals, e.g. images, video, mesh, etc.
* Image Warping
- filtering is altering the *range* of image
- warping is altering the *domain* of image
- Can be useful for many things
  - texture mapping (apply a texture to various 3d surfaces)
  - image processing (rotation, zoom in/out etc)
  - etc
- *Transformation function* used to transform geometry of an image to desired geometry
  - Used to compute corresponding points
- *Control points* Unique points in the reference and target images. The
  coordinates of corresponding control points in images are used to determine a
  transformation function.
** Image warps
- Translation
- Rotation
- Aspect
- Affine
- Perspective
- Cylindrical
** Warping Types
They can be applied globally over a subdivision of the plane
- Piecewise affine over triangulation
- Piecewise projective over a quadrilaterization
- Piecewise bilinear over a rectangular grid
Or other, arbitrary functions can be used, e.g.
- Bieer-neely warp (popular for morphs)
- Store u(x, y) and v(x,y) in large arrays
*** Similarity
- Combination of 2-D scale, rotation, and translation transformations
- Allows a square to be transformed into any rotated rectangle
- Angle between lines is preserved
- 5 degrees of freedom
- Inverse is of same form, given by inverse of transformation matrix
*** Affine mapping
- Combination of 2-D scale, rotation, shear, and translation transformations
- Allows a square to be distorted into any parallelogram
- 6 degrees of freedom
- Inverse is of same form (is also affine). Given by inverse of transformation
  matrix
- Good when controlling a warp with triangles, since 3 points in 2D determined
  the 6 degrees of freedom
*** Projective map
- Linear numerator and denominator
- If g=h=0, then you get affine as a special case
- Allow a square to be distorted into any quadrilateral
- 8 degrees of freedom (the first two columns of the third row of the
  transformation matrix)
- Inverse is of same form
- Good when controlling a warp with quadrilaterals, since 4 points in 2D
  determine the 8 degrees of freedom
** Mapping
- Inverse mapping is better than forward mapping
  - Forward mapping requires that you splat pixels to neighboring pixels
  - Whereas with inverse mapping you simply linearly interpolate.
- Requires a re-sampling filter
- Typically you perform inverse warping followed by a re-sample
** Re-sampling
- HQ resampling requires careful use of low pass filters of variable support
- Good resampling for scale factors near 1 (not scaling up or down much) can
  be done with bilinear interpolation
- Calculated by computing weighted sum of pixel neighborhood
*** Point sampling
- Nearest neighbor
- Copy the color of the closest integer coordinate
- Fast and efficient if the target size is similar to the reference
- Otherwise, the result is chunky, aliased, or blurred
*** Bilinear Filter
- Weighted sum of four neighboring pixels
